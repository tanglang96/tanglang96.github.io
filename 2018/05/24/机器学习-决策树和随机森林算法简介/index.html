<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>[机器学习]决策树和随机森林算法简介 | Math &amp; Code</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="决策树和随机森林算法简介1-决策树1.1-决策树模型的结构决策树（decision tree）是一种分类与回归方法，本文主要讨论用于分类的决策树，决策树的结构呈树形结构，在分类问题中，其代表基于特征对数据进行分类的过程，通常可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型可读性好并且分类速度快。训练的时候，利用训练数据根据损失函数最小化的原则">
<meta name="keywords" content="machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="[机器学习]决策树和随机森林算法简介">
<meta property="og:url" content="http://yoursite.com/2018/05/24/机器学习-决策树和随机森林算法简介/index.html">
<meta property="og:site_name" content="Math &amp; Code">
<meta property="og:description" content="决策树和随机森林算法简介1-决策树1.1-决策树模型的结构决策树（decision tree）是一种分类与回归方法，本文主要讨论用于分类的决策树，决策树的结构呈树形结构，在分类问题中，其代表基于特征对数据进行分类的过程，通常可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型可读性好并且分类速度快。训练的时候，利用训练数据根据损失函数最小化的原则">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://img-blog.csdn.net/20180524210204247?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180524211517468?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180524211653506?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:updated_time" content="2018-08-31T02:19:56.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[机器学习]决策树和随机森林算法简介">
<meta name="twitter:description" content="决策树和随机森林算法简介1-决策树1.1-决策树模型的结构决策树（decision tree）是一种分类与回归方法，本文主要讨论用于分类的决策树，决策树的结构呈树形结构，在分类问题中，其代表基于特征对数据进行分类的过程，通常可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型可读性好并且分类速度快。训练的时候，利用训练数据根据损失函数最小化的原则">
<meta name="twitter:image" content="https://img-blog.csdn.net/20180524210204247?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
  
    <link rel="alternate" href="/atom.xml" title="Math &amp; Code" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Math &amp; Code</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">The best or nothing</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-机器学习-决策树和随机森林算法简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/05/24/机器学习-决策树和随机森林算法简介/" class="article-date">
  <time datetime="2018-05-24T13:22:21.000Z" itemprop="datePublished">2018-05-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      [机器学习]决策树和随机森林算法简介
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="决策树和随机森林算法简介"><a href="#决策树和随机森林算法简介" class="headerlink" title="决策树和随机森林算法简介"></a>决策树和随机森林算法简介</h1><h2 id="1-决策树"><a href="#1-决策树" class="headerlink" title="1-决策树"></a>1-决策树</h2><h2 id="1-1-决策树模型的结构"><a href="#1-1-决策树模型的结构" class="headerlink" title="1.1-决策树模型的结构"></a>1.1-决策树模型的结构</h2><p>决策树（decision tree）是一种分类与回归方法，本文主要讨论用于分类的决策树，决策树的结构呈树形结构，在分类问题中，其代表基于特征对数据进行分类的过程，通常可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型可读性好并且分类速度快。训练的时候，利用训练数据根据损失函数最小化的原则建立决策树模型。预测时对于新的数据，利用决策树进行分类。决策树的学习通常包括三个步骤：特征选择，生成决策树，对决策树进行剪枝。这些决策树的思想主要来自Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法。</p>
<p>用于分类的决策树是一种对数据进行分类的树形结构。决策树主要由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）以及叶节点（leaf node）。内部节点表示一个特征或者属性，叶节点表示一个类。其结构如图所示：<br><img src="https://img-blog.csdn.net/20180524210204247?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="决策树算法的结构"></p>
<h2 id="1-2-特征选择"><a href="#1-2-特征选择" class="headerlink" title="1.2-特征选择"></a>1.2-特征选择</h2><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树的学习效率，如果利用一个特征进行分类的结果与随机分类的结果没有太大差别，则称这个特征是没有分类能力的。通常扔掉这样的特征对于决策树的学习精度影响不大，通常特征选取的准则是信息增益或者信息增益比。</p>
<p>在信息论中，熵（entropy）是表示随机变量不确定性的度量，设X是一个取有限个值得离散随机度量，其概率分布为：</p>
<script type="math/tex; mode=display">
P(X={ {x}_{i} })={ {p}_{i} },i=1,2,...,n</script><p>那么随机变量的熵定义为：</p>
<script type="math/tex; mode=display">
H(X)=-\sum\limits_{i=1}^{n}{ { {p}_{i} }\log { {p}_{i} } }</script><p>熵越大，随机变量的不确定性越大，从定义可以验证</p>
<script type="math/tex; mode=display">
0\le H(p)\le \log n</script><p>当<script type="math/tex">p=0</script>或<script type="math/tex">p=1</script>时<script type="math/tex">H(p)=0</script> ，随机变量完全没有不确定性，当<script type="math/tex">p=0.5</script>时，<script type="math/tex">H(p)=1</script>，熵取值最大，随机变量的不确定性最大。</p>
<p>条件熵<script type="math/tex">H(Y|X)</script>表示在已知随机变量的条件下随机变量的不确定性。随机变量<script type="math/tex">X</script>给定的条件下随机变量<script type="math/tex">Y</script>的条件熵（conditional entropy）<script type="math/tex">H(Y|X)</script>，定义为 <script type="math/tex">X</script>给定条件下<script type="math/tex">Y</script>的条件概率分布的熵对<script type="math/tex">X</script>的数学期望</p>
<script type="math/tex; mode=display">
H(Y|X)=\sum\limits_{i=1}^{n}{ { {p}_{i} }H(Y|X={ {x}_{i} })}</script><p>这里<script type="math/tex">{ {p}_{i} }=P(X={ {x}_{i} }),i=1,2,...,n</script>，信息增益（information gain）表示得知特征<script type="math/tex">X</script>的信息而使得类<script type="math/tex">Y</script>的不确定性减小的程度。</p>
<p>特征<script type="math/tex">A</script>对训练集<script type="math/tex">D</script>的信息增益<script type="math/tex">g(D,A)</script>定义为集合<script type="math/tex">D</script>的经验熵<script type="math/tex">H(D)</script>与特征<script type="math/tex">A</script>给定条件下<script type="math/tex">D</script>的经验条件熵<script type="math/tex">H(D|A)</script>之差，即</p>
<script type="math/tex; mode=display">
g(D|A)=H(D)-H(D|A)</script><h2 id="1-3-构建决策树的ID3算法"><a href="#1-3-构建决策树的ID3算法" class="headerlink" title="1.3-构建决策树的ID3算法"></a>1.3-构建决策树的ID3算法</h2><p>决策树的算法主要有ID3，C4.5以及CART，ID3算法的核心是在决策树各个节点上应用信息增益准则选取特征，递归构建决策树。<br>下面得到构建决策树的ID3算法：<br>输入：训练数据集D，特征集A，阈值<script type="math/tex">\varepsilon</script>；<br>输出：决策树T.</p>
<p>（1）若D中所有实例属于同一类<script type="math/tex">{ {C}_{k} }</script>，则T为单节点树，并将<script type="math/tex">{ {C}_{k} }</script>作为该节点的类标记，返回T；<br>（2）若A=<script type="math/tex">\varnothing</script>，则T为单节点树，并将D中实例数最大的类<script type="math/tex">{ {C}_{k} }</script>作为该节点的类标记，返回T；<br>（3）否则按照信息增益算法计算A中各特征对D的信息增益，选择信息增益最大的特征<script type="math/tex">{ {A}_{g} }</script>；<br>（4）如果<script type="math/tex">{ {A}_{g} }</script>的信息增益小于阈值<script type="math/tex">\varepsilon</script>，则T为单节点树，并将D中实例数最大的类 作为该节点的类标记，返回T；<br>（5）否则，对<script type="math/tex">{ {A}_{g} }</script>的每一可能值<script type="math/tex">{ {a}_{i} }</script>，依据<script type="math/tex">{ {A}_{g} }={ {a}_{i} }</script>将D分割为若干非空子集<script type="math/tex">{ {D}_{i} }</script>，将<script type="math/tex">{ {D}_{i} }</script>中实例数最大的类作为标记，构建子节点，由节点及其子节点够成树T，返回T；对第i个子节点，以<script type="math/tex">{ {D}_{i} }</script>为训练集，以<script type="math/tex">A-\{ { {A}_{g} }\}</script>为特征集，递归地调用步1-步5，得到子树<script type="math/tex">{ {T}_{i} }</script>，返回<script type="math/tex">{ {T}_{i} }</script>。</p>
<h2 id="2-集成学习"><a href="#2-集成学习" class="headerlink" title="2-集成学习"></a>2-集成学习</h2><p>集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务，有时候也被称为多分类器系统（multi-classfier system）、基于委员会的学习（committee-based learning）等。</p>
<p>下图显示出集成学习的一般结构：先构建一组“个体学习器”（individual learner），再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法由训练数据产生，例如ID3决策树算法，BP神经网络算法等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质”的（homogeneous）。同质集成中的个体学习器也称为“基学习器”（base learner），相应的学习算法称为“基学习算法”（base learning algorithm）。集成也可包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是“异质”的（heterogenous）。异质集成中的个体学习器由不同的学习算法产生，这是就不再有基学习算法；相应的，个体学习器一般不称为基学习器，常称为“组件学习器”（component learner）或直接称为个体学习器。<br><img src="https://img-blog.csdn.net/20180524211517468?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="集成学习器"></p>
<p>集成学习通过将多个学习器进行结合，常常可以获得比单一学习器更为显著优越的泛化性能。这对“弱学习器”（weak learner）尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的，而基学习器有时也被直接成为弱学习器。但是需要注意的是，虽然从理论上来说使用弱学习器集成足以获得好的性能，但在实践中出于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器。</p>
<p>根据个体学习器的生成方式，目前的集成学习方法大致可分为两类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和“随机森林”（Random Forest）。</p>
<h2 id="3-随机森林算法"><a href="#3-随机森林算法" class="headerlink" title="3-随机森林算法"></a>3-随机森林算法</h2><p>随机森林（Random Forest，简称RF）[Breiman,2001a]是Bagging的一个扩展变体，RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前节点的属性集合（假定有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令k=d，则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性用于划分；一般情况下，推荐值<script type="math/tex">k={ {\log }_{2} }d</script>。</p>
<p>随机森林简单，容易实现，计算开销小，令人惊奇的是，它在很多现实任务中表现出了强大的性能，被誉为“<strong>代表集成学习技术水平的方法</strong>”。可以看出，随机森丽对Bagging只做了小改动，但是与Bagging中基学习器的“多样性”仅通过样本扰动而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步提升，决策树的结构如下图：<br><img src="https://img-blog.csdn.net/20180524211653506?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>对于随机森林的编程，可以使用scikit-learn框架<br><a href="http://scikit-learn.org/stable/user_guide.html#" target="_blank" rel="noopener">http://scikit-learn.org/stable/user_guide.html#</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/05/24/机器学习-决策树和随机森林算法简介/" data-id="cjzavx0zz00246gawpbbvsr7g" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/05/25/Cpp-Cpp中的顺序容器/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [Cpp]Cpp中的顺序容器
        
      </div>
    </a>
  
  
    <a href="/2018/05/18/最优化-等式约束的优化问题求解/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[最优化]等式约束的优化问题求解</div>
    </a>
  
</nav>

  
</article>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script> 
<div id="gitalk-container"></div>     <script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '2193b885b967a685e247',
        clientSecret: '49a60c8f19873532dc3e7a6658bbff6c95ce73c9',
        repo: 'Lannyy.github.io',
        owner: 'Lannyy',
        admin: ['Lannyy'],
        id: '2018/05/24/机器学习-决策树和随机森林算法简介/',
        distractionFreeMode: true,
    });
    gitalk.render('gitalk-container');
</script></section>
        
          <aside id="sidebar">
  
    
<div class="widget-wrap">
  <h3 class="widget-title">ABOUT ME</h3>
  <ul class="widget about-me">
    
    <li><img class="author" title="About me" src="/images/cute.jpg" /></li>
    
    
    <li>Name：Tang Lang</li>
    
    <li>School：Xiamen University</li>
    
    <li>Email：langt@stu.xmu.edu.cn</li>
    
    <li>QQ：1660039482</li>
    
  </ul>
</div>


  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cpp/">Cpp</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matrix/">Matrix</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a><span class="tag-list-count">20</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/optimization/">optimization</a><span class="tag-list-count">9</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Cpp/" style="font-size: 15px;">Cpp</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/algorithm/" style="font-size: 12.5px;">algorithm</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/optimization/" style="font-size: 17.5px;">optimization</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/17/paper-NAS-conclusion/">[machine learning]NAS conclusion</a>
          </li>
        
          <li>
            <a href="/2018/11/22/paper-robot/">[paper]robot</a>
          </li>
        
          <li>
            <a href="/2018/11/17/paper-dqn/">[paper]dqn</a>
          </li>
        
          <li>
            <a href="/2018/11/02/dataset-cityscapes/">dataset-cityscapes</a>
          </li>
        
          <li>
            <a href="/2018/10/30/paper-EDAnet/">[paper]EDAnet</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Tang Lang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>