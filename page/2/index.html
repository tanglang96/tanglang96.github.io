<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Math &amp; Code</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="the best or nothing">
<meta name="keywords" content="Algorithm">
<meta property="og:type" content="website">
<meta property="og:title" content="Math &amp; Code">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Math &amp; Code">
<meta property="og:description" content="the best or nothing">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Math &amp; Code">
<meta name="twitter:description" content="the best or nothing">
  
    <link rel="alternate" href="/atom.xml" title="Math &amp; Code" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Math &amp; Code</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">---By Frank,the best or nothing</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-paper-ShuffleNet" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/12/paper-ShuffleNet/" class="article-date">
  <time datetime="2018-10-12T01:28:55.000Z" itemprop="datePublished">2018-10-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/12/paper-ShuffleNet/">[paper]ShuffleNet</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="《ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices》总结"><a href="#《ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices》总结" class="headerlink" title="《ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices》总结"></a>《ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices》总结</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文介绍了一种很高效的网络ShuffleNet，其主要在于pointwise group conv和channel shuffle两种操作，可以在维持精度的时候大量减少计算消耗，在ImageNet和COCO上面的表现都超过了之前的网络</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>构造更深，更宽的神经网络在解决视觉识别上面是发展趋势。然而本文提出另外一个极端：在有限的计算资源下达到最佳的性能。现有的很多方法（剪枝压缩等）都只是在处理基本的神经网络，而这里提出了一种给定计算资源下的高效网络结构。</p>
<p>现在的state-of-the-art网络对资源都比较消耗计算资源，主要是因为太密集的1x1 conv，本文使用了1x1 group conv来减小计算量，并且使用channel shuffle来增加各个channel之间的通讯，这样可以编码更多信息，基于这两个技术而构建了ShuffleNet，基于这两个设计准则，在ImageNet和COCO上都取得了更好的性能，另外在真实的硬件平台上也有明显的加速效果</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h2><p><strong>Efficient Model Designs</strong>：GoogLeNet增加了网络深度但是相比简单的conv层堆积减少了计算量。SqueezeNet在维持精度的情况下明显减少了计算量。ResNet利用有效的瓶颈结构来得到较高的性能。SENet引入了一个结构单元在小计算量的情况下加速性能。另外最近有工作利用强化学习和模型搜索来探索高效的模型设计，得到的NASNet得到了不错的精度表现，但是在较小的FLOPs下表现比较一般。</p>
<p><strong>Group Convolution</strong>：group conv最开始在AlexNet当中被提出，当时是为了在多个GPU上进行分布式训练，后来在ResNeXt当中表现了其有效性。Xception中的Depthwise separable conv总结了Inception系列网络的想法，后来MobieNet利用depthwise separable conv实现了state-of-the-art。</p>
<p><strong>Channel Shuffle Operations</strong>：channel shuffle的思想在之前的工作中很少被提出来，即使cuda-convnet支持”random sparse conv”层，等价于后面带有group conv层的random channel shuffle。最近有工作在two-stage conv上应用这个思想但是却没有channel shuffle自身在小模型设计上的有效性。</p>
<p><strong>Model Acceleration</strong>：这个是为了在保留模型精度的时候加速推断，对网络连接或者通道进行剪枝可以在保留性能的时候去掉冗余的连接。quantization和factorization可以减少计算当中的冗余从而加速推断。另外在不修改参数的时候，优化的卷积算法比如FFT都会减少实际当中的时间消耗。另外distilling可以从大模型中训练小模型，可以让训练小模型变得更加容易。</p>
<h2 id="3-Approch"><a href="#3-Approch" class="headerlink" title="3.Approch"></a>3.Approch</h2><h3 id="3-1-Channel-Shuffle-for-Group-Convolutions"><a href="#3-1-Channel-Shuffle-for-Group-Convolutions" class="headerlink" title="3.1 Channel Shuffle for Group Convolutions"></a>3.1 Channel Shuffle for Group Convolutions</h3><p>现在的很多state-of-the-art网络，比如Xception和ResNeXt都有太多的1x1 conv，在每个block中占了大量的计算量，如果在限制计算资源的情况下就只能有更少的channel数，这样会降低模型的精度。要解决这个问题可以使用channel之间的稀疏连接，例如group conv就是其中的一个例子。然而这样会导致一个组内的输出只与这个组内的输入相关，会导致channel group之间的联系变得较弱，可以通过channel shuffle可以让group conv从不同的group获取信息，从而解决这个问题。具体操作为(g,n)维度先转置，然后展开，然后再reshape为(g,n)，其中g为group数，n为每组channel数。当两个卷积有不同的group数的时候依然是有效的，并且这个shuffle操作也是可以微分的。<img src="/images/paper-ShuffleNet/channel-shuffle.PNG" alt="Channel shuffle"></p>
<h3 id="3-2-ShuffleNet-Unit"><a href="#3-2-ShuffleNet-Unit" class="headerlink" title="3.2 ShuffleNet Unit"></a>3.2 ShuffleNet Unit</h3><p>这个unit是专门为小网络设计的，使用了bottleneck的结构。unit分为两种，一种是没有stride，输入输出维度一样的，采用残差结构，最后是求和。另外一种是有stride的，同样残差结构不过最后采用了concat。这个Unit相比ResNet和ResNeXt更加高效，比如给定输入维度<script type="math/tex">c*h*w</script>，瓶颈通道数为<script type="math/tex">m</script>，ResNet的FLOPs为<script type="math/tex">hw(2cm+9m^{2})</script>，ResNeXt的FLOPs为<script type="math/tex">hw(2cm+9m^2/g)</script>，ShuffleNet unit的FLOPs为<script type="math/tex">hw(2cm/g+9m)</script>，也就相当于在给定计算资源的情况下ShuffleNet unit能够使用更宽的特征图。另外depthwise conv虽然只有很小的理论上的复杂度，但是FLOPs/MAC比较小，这个在后面的ShuffleNetV2中有提到，所以只在bottleneck那里使用depthwise conv。<img src="/images/paper-ShuffleNet/ShuffleNet-unit.PNG" alt="ShuffleNet unit"></p>
<h3 id="3-3-Network-Architecture"><a href="#3-3-Network-Architecture" class="headerlink" title="3.3 Network Architecture"></a>3.3 Network Architecture</h3><p>在整个网络里面，为了简化，Bottleneck的channel数为输入的<script type="math/tex">1/4</script>，这里提供一个尽可能简单的参考模型，尽管进一步调参可能会取得更好的结果。其中的参数<script type="math/tex">g</script>控制连接的稀疏度，更多的group可能会帮助编码更多信息，但是对于单个conv可能会降低其性能，在ShuffleNetV2中也从另一个角度讨论了该问题：过多的group会增加MAC从而降低速度。另外用一个超参数<script type="math/tex">s</script>来对网络进行缩放，通过缩放调整通道数来实现。<img src="/images/paper-ShuffleNet/architecture.PNG" alt="ShuffleNet网络结构"></p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4.Experiments"></a>4.Experiments</h2><p>主要在ImageNet上做的实验，因为小型网络更加容易遭遇过拟合，所以使用了很轻微的data augmentation。</p>
<h3 id="4-1-1-Ablation-Study"><a href="#4-1-1-Ablation-Study" class="headerlink" title="4.1.1 Ablation Study"></a>4.1.1 Ablation Study</h3><p>这里主要对于两个方面进行分析，一是pointwise group conv，二是channel shuffle。这两个是ShuffleNet的核心组件</p>
<p>对于pointwise group conv，这里利用不同规模(缩放s)，不同group的网络进行对比，实验表明对于比较大的网络，当group达到一定值之后对于单个conv，其输入的通道数过少，将会影响表达性能。但是当缩放因子比较小的时候，网络比较小在group增大的时候更能明显提高性能，因为对于小网络而言，更宽的特征图可以带来更大的好处。<img src="/images/paper-ShuffleNet/group-ex.PNG" alt="1x1 group conv实验"></p>
<p>对于channel shuffle，实验证明channel shuffle明显能够持续提高分类分数，尤其当group比较大的时候性能更好，此时的跨组信息交换变得更加重要。<img src="/images/paper-ShuffleNet/shuffle-ex.PNG" alt="shuffle实验"></p>
<h3 id="4-2-Comparison-with-Other-Structure-Units"><a href="#4-2-Comparison-with-Other-Structure-Units" class="headerlink" title="4.2 Comparison with Other Structure Units"></a>4.2 Comparison with Other Structure Units</h3><p>在之前的ShuffleNet框架上与一些unit进行了对比，实验结果如图<img src="/images/paper-ShuffleNet/unit-ex.PNG" alt="不同unit"></p>
<h3 id="4-3-Comparison-with-MobileNets-and-Other-Frameworks"><a href="#4-3-Comparison-with-MobileNets-and-Other-Frameworks" class="headerlink" title="4.3 Comparison with MobileNets and Other Frameworks"></a>4.3 Comparison with MobileNets and Other Frameworks</h3><p>这里主要将ShuffleNet与其它一些经典的结构相比较，其中值得注意的是在ShuffleNet和MobileNet相比较的过程中，减少ShuffleNet的深度也明显会比MobileNet更好，这说明ShuffleNet与之相比关键在于unit的设计而非网络深度，另外可以将ShuffleNet与其他优秀设计比如SE模块（Squeeze-and-Excitation）相组合，可以进一步提升其性能但是会变得比较慢。<img src="/images/paper-ShuffleNet/mobile-ex.PNG" alt="与MobileNet相比"></p>
<p><img src="/images/paper-ShuffleNet/other-ex.PNG" alt="与其他结构相比"></p>
<h3 id="4-4-Generalization-Ability"><a href="#4-4-Generalization-Ability" class="headerlink" title="4.4 Generalization Ability"></a>4.4 Generalization Ability</h3><p>为了探究ShuffleNet的泛化和迁移学习的性能，在COCO上进行了测试，利用Faster-RCNN的框架来进行实验。在这项任务上依旧比MobileNet好了很多，可能是由于网络结构设计没有过多的冗余装饰，所以泛化性能比较好。<img src="/images/paper-ShuffleNet/coco-ex.PNG" alt="关于泛化性能的实验"></p>
<h3 id="4-5-Actual-Speedup-Evaluation"><a href="#4-5-Actual-Speedup-Evaluation" class="headerlink" title="4.5 Actual Speedup Evaluation"></a>4.5 Actual Speedup Evaluation</h3><p>这里在ARM平台上进行了测试，group数较大的时候（比如g=8）性能最好但是g=3的时候在性能和推断时间上有着比较好的平衡，但是依旧要比其他网络更优秀。<img src="/images/paper-ShuffleNet/speed-ex.PNG" alt="实际的速度测试"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/10/12/paper-ShuffleNet/" data-id="cjomnvjpz000t40awmulk0t7r" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper-ShuffleNetV2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/11/paper-ShuffleNetV2/" class="article-date">
  <time datetime="2018-10-11T02:19:56.000Z" itemprop="datePublished">2018-10-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/11/paper-ShuffleNetV2/">[paper]ShuffleNetV2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="《ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design》总结"><a href="#《ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design》总结" class="headerlink" title="《ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design》总结"></a>《ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design》总结</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>现在很多的网络设计在计算复杂度方面都只考虑了非直接度量（比如FLOPs），而对于直接度量（如速度等）并不只是由FLOPs来决定的，包括MAC（内存访问消耗）和平台特性都对速度有一定的影响。本文意在特定平台下进行直接度量，比仅仅考虑FLOPs要更好，并且在基于一系列控制实验下提出了一些对于高效率网络的指导准则，根据这些准则提出了ShuffleNetV2这样一种新的网络结构，全面的ablation experiments表明该模型在性能和精度的权衡上达到了state-of-the-art。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>自从AlexNet在ImageNet上取得了很好的成绩之后，ImageNet上的分类准确度又被一些新型的神经网络进一步改善，比如VGG，GoogLeNet，ResNet，DenseNet，ResNeXt，SE-Net等，而且还有<strong>自动网络结构搜索</strong>。除了精度之外，计算复杂度也是另外一个需要考虑的因素，现实世界的应用通常会受到很多平台的限制，这促进了很多轻量级神经网络结构的设计，以及更好的速度-精度平衡，比如Xception，MobileNet，MobileNetV2，ShuffleNet和CondenseNet等。在这些工作里面，group conv和depth-wise conv是至关重要的。</p>
<p>为了评价计算复杂度，常用的评价标准就是FLOPs（在本文中的定义是mult-adds的数量），然而FLOPs只是一个近似的间接度量，而不是我们所关心的直接度量，例如速度，延迟等。这样的矛盾在最近的工作中已经被注意到了。比如MobileNetV2和NASNET-A有相近的FLOPs但是MobileNetV2的速度要快得多。下图说明模型有近似的FLOPs但是可能有不同的速度<img src="/images/paper-ShuffleNetV2/FLOPs-speed.PNG" alt="近似的FLOPs具有不同的速度">，因此仅仅使用FLOPs作为度量是不够的，并且有可能导致次优设计。</p>
<p>在间接度量（FLOPs）和直接度量（速度）之间的矛盾主要来自于两个方面（<strong>度量和平台</strong>）：一是很多关键的影响速度的因素没有考虑进来，比如MAC（memory access cost，group conv是其中一个重要的组成，对于GPU这样强大的运算单元来说这可能会变成其瓶颈），还有就是DOP（degree of parallelism，在同样的FLOPs下，高DOP的模型可能要比低DOP的模型快很多）。第二个原因就是对于平台的依赖，比如张量分解这样的操作，尽管能够减少75%的FLOPs，但是在分解后CPU上的运行速度甚至比GPU更慢，后来发现这是由于最近的CUDNN库对于3x3卷积有特殊的优化，不能简单地认为3x3 conv就比1x1 conv要慢9倍。</p>
<p>通过这些观察，发现在高效的神经网络设计中应该遵循两条原则，一是要使用直接度量而非间接度量，二是要在特定的平台上进行度量。基于这两条准则提出了更加有效的神经网络结构。在第二章中我们首先分析了两个代表性的state-of-the-art神经网络（ShuffleNetV1，MobileNetV2）的运行时性能，之后我们提出了高效网络设计的四条准则，比只考虑FLOPs更进一步。然而这些准则是平台独立的，我们进行了不同的控制实验来在两个平台上（GPU和ARM）验证它们（带有代码优化），来保证我们的结论是state-of-the-art。</p>
<p>在第三章节中设计了ShuffleNetV2，通过第四章完整的验证实验，证明其在两个平台上都比之前的网络要更加准确和快速。</p>
<h2 id="2-Practical-Guidelines-for-Efficient-Network-Design"><a href="#2-Practical-Guidelines-for-Efficient-Network-Design" class="headerlink" title="2.Practical Guidelines for Efficient Network Design"></a>2.Practical Guidelines for Efficient Network Design</h2><p>本研究是基于两个带有工业级优化的CNN库的被广泛应用的硬件平台上进行。注意到我们的CNN库要比大多数开源库更加高效，因此我们的观察和结论是牢固的并且在工业上是有实践价值的。GPU（单个GTX1080TI，卷积库是CUDNN7.0，并且激活了CUDNN的benchmarking函数来对于不同的卷积选择最快的算法），ARM（高通骁龙810，基于Neon实现，利用单线程来进行验证），其他的一些设置：开启全优化（张量融合之类的），输入图片尺寸：224x224，所有网络进行随机初始化，并且评估100次取平均运行时间。</p>
<p>对于刚开始的研究我们选取MobileNetV2和ShuffleNetV1，尽管只有两个，但是这代表了最近的趋势，这两个网络的核心是group conv和depth-wise conv，这也同样是其他state-of-the-art网络的核心。整个运行时分解图如下：<img src="/images/paper-ShuffleNetV2/runtime-decompose.PNG" alt="运行时间分解图">，FLOPs只算了卷积部分，尽管其占了大部分时间，但是一些其他的内容比如：数据I/O，data shuffle和element-wise操作也占了很大一部分，因此FLOPs用作时间的估计是很不准确的。基于以上观察，我们对运行时间从几个方面进行了分析，并且得出了对于高效神经网络设计的一些实际经验。</p>
<h3 id="G1-Equal-channel-width-minimize-memory-access-cost"><a href="#G1-Equal-channel-width-minimize-memory-access-cost" class="headerlink" title="G1.Equal channel width minimize memory access cost"></a>G1.Equal channel width minimize memory access cost</h3><p>很多现代网络都采用depth-wise conv，其中1x1卷积占了大部分复杂度，其FLOPs为<script type="math/tex">B=hwc_1c_2</script>，假设内存够大足够存储输入输出的特征和卷积核权重，MAC为<script type="math/tex">MAC=hwc_1+hwc_2+c_1c_2</script>，即为输入特征+输出特征+卷积核权重。通过均值不等式可以得到<script type="math/tex">MAC\ge 2\sqrt{hwB}+\frac{B}{hw}</script>，当<script type="math/tex">c_1=c_2</script>时该不等式取等号，即当输出通道数等于输入通道数时MAC最小。当然这只是理论上的，利用重复堆积10个block来作为评价网络，通过调整通道数来使整个FLOPs保持不变，实际的实验结果如下：<img src="/images/paper-ShuffleNetV2/channels-benchmark.PNG" alt="不同通道数的速度"></p>
<h3 id="G2-Excessive-group-convolution-increase-MAC"><a href="#G2-Excessive-group-convolution-increase-MAC" class="headerlink" title="G2.Excessive group convolution increase MAC"></a>G2.Excessive group convolution increase MAC</h3><p>group conv是很多现代网络的核心，可以通过channel之间的稀疏连接来减少FLOPs，一方面可以在固定FLOPs下使用更多的channel，以此增加网络容量（从而改善精度），另一方面增加channel也会增加MAC。根据上一条里面的公式可以得到1x1 conv的FLOPs和MAC之间的关系为：<script type="math/tex">MAC=hwc_1+\frac{Bg}{c_1}+\frac{B}{hw}</script>，其中g为组数，可以看到在h，w，c1，c2，B固定的情况下，增加g会增加MAC。通过堆积10个1x1 group conv来构造评价网络，结果证明一味选取很大的group数并不好，其带来的精度提升的好处可能会被增加的计算消耗所抵消掉。实验结果如下：<img src="/images/paper-ShuffleNetV2/group-speed.PNG" alt="不同组数之间的对比">。</p>
<h3 id="G3-Network-fragmentation-reduces-degree-of-parallelism"><a href="#G3-Network-fragmentation-reduces-degree-of-parallelism" class="headerlink" title="G3.Network fragmentation reduces degree of parallelism"></a>G3.Network fragmentation reduces degree of parallelism</h3><p>在GoogLeNet系列和自动生成结构系列，“多路径”结构在每个网络block被广泛应用，许多小操作（fragmented operators）在这里被应用，而不是少量的大操作。比如在NASNET-A中fragmented ops（比如某个block中单独的卷积/池化操作）是13，而在一些正则化结构中（比如ResNet）是2或3。这样的碎片结构已经表明对准确度有利，但是可能会降低效率因为这些操作对于一些高度并行计算的设备比如GPU很不友好。也会导致核的启动/同步等额外开销。为了对此影像进行量化，设计了实验block如下：<img src="/images/paper-ShuffleNetV2/DOP-net.PNG" alt="DOP实验的网络">，每个block重复10次。试验结果表明碎片化会明显降低了GPU上的速度，相对GPU，ARM上的速度降低更轻微一些。实验结果如图：<img src="/images/paper-ShuffleNetV2/DOP-speed.PNG" alt="网络碎片对速度的影响">。</p>
<h3 id="G4-Element-wise-operations-are-non-negligible"><a href="#G4-Element-wise-operations-are-non-negligible" class="headerlink" title="G4.Element-wise operations are non-negligible"></a>G4.Element-wise operations are non-negligible</h3><p>如之前的图片所示，element-wise操作（张量相加，bias，ReLU等）占据了相当的时间，尤其是在GPU上面，它们具有比较小的FLOPs但是具有较大的MAC，尤其是depthwise conv作为element-wise操作，因为其具有较大的MAC/FLOPs比率。在bottleneck结构上进行了实验，针对ReLU和shortcut：<img src="/images/paper-ShuffleNetV2/elementwise-speed.PNG" alt="逐点操作的实验结果"></p>
<h3 id="Conclusion-and-Discussion"><a href="#Conclusion-and-Discussion" class="headerlink" title="Conclusion and Discussion"></a>Conclusion and Discussion</h3><p>总结：1)应该使用尽量平衡的卷积（输入输出通道相等），2)了解group-conv的消耗，3)减少网络的碎片度，3)减少逐点操作。而且相比理论上而言，更应该考虑到网络在平台上的特性，并且应用在实际网络设计当中。之前的很多网络违背了这些规则，例如ShuffleNetV1太依赖group conv违背了G2，其瓶颈结构设计违背了G1，MobileNetV2的瓶颈设计违背了G1，在太厚的特征图上使用ReLU等违背了G4，自动生成的网络结构太过碎片化违背了G3。</p>
<h2 id="3-ShuffleNet-V2：an-Efficient-Architecture"><a href="#3-ShuffleNet-V2：an-Efficient-Architecture" class="headerlink" title="3.ShuffleNet V2：an Efficient Architecture"></a>3.ShuffleNet V2：an Efficient Architecture</h2><p>轻量级网络设计的主要挑战是给定计算量下的有限的通道数。想在有限的FLOPs下增加通道数的办法有两个：1)pointwise group conv，2)瓶颈结构。之后又引出了channel shuffle这样的结构来增加不同channel的通讯。但是pointwise group conv和瓶颈结构都有违背之前的准则，现在的问题在于如何维持大量的平均匀分布的channel数，而不至于太密的卷积或者太多的组数。</p>
<p>对于以上的需求做了一些设计：刚开始就对channel进行分割，然后在1x1 conv中不再使用group，也不使用瓶颈结构，而是让输入输出channel相等，并且只在一个支路上进行ReLu和depthwise conv。最后进行concat和channel shuffle。并且将concat，channel shuffle，split合并成一个element-wise操作。与V1相同可以利用s来对网络进行缩放，通过改变通道数，另外在进行split的时候为了简单起见采用了对半分的方式。ShuffleNetV2 block的结构如下：<img src="/images/paper-ShuffleNetV2/block-structure.PNG" alt="ShuffleNetV1和V2的block对比"></p>
<p>ShuffleNetV2很高效，因此可以具有更多的通道数和模型容量，另外特征的对半分可以看作一种feature reuse，类似于DenseNet和CondenseNet。DenseNet和ShuffleNetV2在feature reuse上面的模式对比如下图：<img src="/images/paper-ShuffleNetV2/feature-reuse.PNG" alt="feature reuse的模式对比">ShuffleNetV2在feature reues上获取了和DenseNet同样的好处，却相比更加高效，后续的实验可以说明。</p>
<h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4.Experiment"></a>4.Experiment</h2><p>实验所使用的超参数和协议与ShuffleNetV1完全相同，在这里先放上实验结果，后面再对实验进行总结：<img src="/images/paper-ShuffleNetV2/experiments.PNG" alt="实验结果"></p>
<p><strong>Accuracy vs. FLOPs</strong>：ShuffleNetV2明显可以超过其他所有网络，但是在40MFLOPs，224x224图像尺寸的时候由于通道数太少表现也欠佳。该模型和DenseNet相比，都有reuse features，但是本模型更高效。</p>
<p><strong>Inference Speed vs. FLOPs/Accuracy</strong>：MobileNetV2在小FLOPs的时候非常慢，可能是由于具有过高的MAC。尽管MobileNetV1在准确度上欠佳，但是其在GPU上的面的速度很快，已经超过了ShuffleNetV2，可能是其更加满足之前的设计准则，尤其是G3，MobileNetV1的fragments是比ShuffleNetV2还要更小的。另外，IGCV2和IGCV3都很慢，可能是由于使用了过多的group conv，然后这些现象都与我们的设计准则相符合。目前的自动搜索出的神经网络结构都比较慢，应该是具有较多的fragments，这违背了G3，不过这个研究方向仍然是充满前景的。在accuracy vs. speed这个方面，ShuffleNetV2在GPU和CPU两个平台上都是表现最好的。</p>
<p><strong>Compatibility with other methods</strong>：将ShuffleNetV2和其他结构相结合的时候，比如SE（squeeze and excitation），分类准确率有所提升，应该说明了其良好的兼容性。</p>
<p><strong>Generalization to Large</strong> ：当ShuffleNetV2用作大模型的时候性能也很好，只不过在做得很深的时候为了加快收敛速度而增加了残差连接</p>
<p><strong>Object Detection</strong>：在COCO数据集上来评估其泛化性能，使用state-of-the-art的轻量级检测器Light-Head RCNN来作为框架，在ImageNet上进行了训练然后在finetuned用作检测任务。后来发现Xception比较擅长检测任务，可能是由于block更大的接受域，收到这个的启发，我们在第一个1x1 conv之前加入了3x3的depthwise conv，最后在增加了一点FLOPs的情况下更好的改善了性能。另外在检测任务下，不同模型之间的速度差要小于分类任务（除去数据拷贝的开销和检测特有的开销）。ShuffleNetV2*有最好的精度并且比其它方法都要快，这启发出了另一个实际问题：如何让增加接受域的大小，这在高分辨率图像的目标检测中是至关重要的。</p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5.Conclusion"></a>5.Conclusion</h2><p>网络设计应该考虑直接度量而非间接度量，提供了结构设计的有效准则和一种高效的神经网络ShuffleNetV2，全面的实验充分证明了模型的有效性，希望这项工作能引出对于平台相关而且更加实际的网络设计。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/10/11/paper-ShuffleNetV2/" data-id="cjomnvjq1000w40awcdd9sq3c" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper-MobileNets" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/04/paper-MobileNets/" class="article-date">
  <time datetime="2018-10-04T12:34:54.000Z" itemprop="datePublished">2018-10-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/04/paper-MobileNets/">[paper]MobileNets</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="paper-《MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision"><a href="#paper-《MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision" class="headerlink" title="[paper]《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision"></a>[paper]《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision</h1><p>Applications》总结</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>针对移动和嵌入式视觉应用，本文提出了一种高效的模型称之为MobileNets，基于depthwise separable convolutions构造的一种轻量级神经网络。该模型使用两个超参数来平衡准确率和延迟，并针对二者的平衡在ImageNet上做了广泛的实验，与其他模型相比展现出了强大的性能。并通过实验展现了ImageNet在各种应用上的强大之处，包括目标检测，精细化分类，人脸属性和大范围地理定位等。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1-Introduction"></a>1-Introduction</h2><p>自从AlexNet使深度CNN变得流行起来，CNN在计算机视觉方面变得无所不在，总体趋势在于发明更深更复杂的网络来实现更高的精度。然而这些改进在网络速度和尺寸上并没有起到促进作用，显示应用中的机器人，无人驾驶汽车，AR等都需要在有限的计算平台下具有实时性。</p>
<p>本文提出了一种高效的网络结构和两个超参数构成的集合来构建用于以上应用的模型，章节2审视了前人在构建小型模型方面的经验，章节3描述了MobileNet的结构和宽度乘子，分辨率乘子这两个超参数，章节4描述了其在ImageNet和各种应用下的实验，最后章节5进行了总结。</p>
<h2 id="2-Prior-Work"><a href="#2-Prior-Work" class="headerlink" title="2-Prior Work"></a>2-Prior Work</h2><p>构建高效的小尺寸网络在最近很流行，很多方法能被分类成两种：压缩欲训练网络或者直接训练小尺寸网络，本文提出的网络结构可以让搭建者来选择满足资源约束的小尺寸网络，MobileNet主要专注于优化延迟并产出小尺寸网络，许多网络都只考虑了尺寸而没有速度。 </p>
<p>MobileNets利用depthwise separable convolutions，Inception model也利用这个来减少前几层的计算量。Flattened networks利用全分解卷积来构建网络，并且展现了分解网络的潜力。Factorized networks利用了相似的卷积分解，也利用了topological connections。另外的还有Xception network：通过缩放depthwise separable filters。Squeezenet：利用bottleneck。其他一些减少计算量的网络：structured transform network，deep fried convnets。</p>
<p>获得小尺寸网络的其他方法：对于欲训练网络的shrinking，factorizing，compressing，其中compressing包含：product quantization，hashing，pruning，vector quantization，Huffman coding。其中factorization的方法：[14,20]文献。另外的方法还有distillation(用大网络的输出来训练小网络)，low bit networks。</p>
<h2 id="3-MobileNet-Architecture"><a href="#3-MobileNet-Architecture" class="headerlink" title="3-MobileNet Architecture"></a>3-MobileNet Architecture</h2><p>这一章首先介绍核心的depthwise separable filters，然后介绍MobileNet的网络结构，最后以shrinking的两个超参数（宽度乘子，分辨率乘子）的介绍结尾。</p>
<h3 id="3-1-Depthwise-Separable-Convolutions"><a href="#3-1-Depthwise-Separable-Convolutions" class="headerlink" title="3.1-Depthwise Separable Convolutions"></a>3.1-Depthwise Separable Convolutions</h3><p>depthwise separable convolution是将一个标准卷积分成了两部分：depthwise convolution和1x1的卷积。同时也将一个卷积层分成了两层：filtering和combining。这样的分解可以极大地减少计算量和模型尺寸。标准卷积和depthwise separable convolution的对比如下图：<img src="\images\paper-MobileNets\depthwise-separable-convolution.PNG" alt="两种卷积方式的区别"></p>
<p>对于一个标准卷积，假设其输入维度为<script type="math/tex">D_{F}*D_{F}*M</script>，输出维度为<script type="math/tex">D_{G}*D_{G}*N</script>，卷积核的维度为<script type="math/tex">D_{K}*D_{K}*M*N</script>，那么在stride=1，padding的情况下，标准卷积的计算公式为：</p>
<script type="math/tex; mode=display">
G_{k,l,n}=\sum_{i,j,m}K_{i,j,m,n}\cdot F_{k+i-1,l+j-1,m}</script><p>标准卷积的计算消耗为：</p>
<script type="math/tex; mode=display">
D_{K}\cdot D_{K}\cdot M\cdot N\cdot D_{F}\cdot D_{F}</script><p>而depthwise separable convolution的计算消耗为：</p>
<script type="math/tex; mode=display">
D_{K}\cdot D_{K}\cdot M\cdot D_{F}\cdot D_{F}+M\cdot N\cdot D_{F}\cdot D_{F}</script><p>前者为depthwise convolution的计算消耗，而后者为1x1的卷积的计算消耗，通过比较，二者的计算量之间的减少为：</p>
<script type="math/tex; mode=display">
\frac{D_{K}\cdot D_{K}\cdot M\cdot D_{F}\cdot D_{F}+M\cdot N\cdot D_{F}\cdot D_{F}}{D_{K}\cdot D_{K}\cdot M\cdot N\cdot D_{F}\cdot D_{F}}=\frac{1}{N}+\frac{1}{D_{K}^{2}}</script><p>如果使用3x3的卷积核，那么计算量将减小8-9倍，并且在准确率上只有微小的降低。再进一步进行分解[16,31]并不会减少很多计算量因为depthwise convolution的计算量已经很小了。</p>
<h3 id="3-2-Network-Structure-and-Training"><a href="#3-2-Network-Structure-and-Training" class="headerlink" title="3.2-Network Structure and Training"></a>3.2-Network Structure and Training</h3><p>MobileNet除了第一层是标准卷积之外，其余结构都是基于depthwise separable convolution来构建的。整个网络结构如下图：<img src="\images\paper-MobileNets\MobileNet-Body-Architecture.PNG" alt="MobileNet的网络结构"></p>
<p>值得一提的是，并不能以小数量的Mult-Adds就认为这个模型是高效的。让这些Mult-Adds操作能够高效实现同样也很重要。比如非结构化的稀疏矩阵操作并不一定比密集矩阵的操作更快，除非具有很高的稀疏度。我们的模型几乎将所有的计算转化为密集的1x1卷积操作，这种操作可以用一种经过高度优化的通用矩阵乘法（GEMM）来实现。通常的用GEMM实现的卷积操作需要先使用im2col来对输入在内存中重新进行排序，例如这样的操作可以用Caffe来实现。而我们的1x1卷积则不需要先排序，可以直接应用GEMM算法（最优的数值线性代数算法之一）。在MobileNet中，95%的Mult-Adds操作和75%的参数都来自1x1卷积。</p>
<p>对于训练的细节：TensorFlow+RMSprop+asynchronous gradient descent(类似InceptionV3)+更少的正则化和数据增强(小模型不容易过拟合)+很少或者没有weight decay on the depthwise filters(因为里面已经只有很少的参数)</p>
<h3 id="3-3-Width-Multiplier-Thinner-Models"><a href="#3-3-Width-Multiplier-Thinner-Models" class="headerlink" title="3.3-Width Multiplier: Thinner Models"></a>3.3-Width Multiplier: Thinner Models</h3><p>尽管当前的MobileNet已经很小很快了，不过有时候还需要更小的模型，我们引入一个超参数<script type="math/tex">\alpha</script>(width multiplier)来构建这些更小的模型，这个参数的目标是均匀的在每一层来让整个网络变得更加瘦小。给定一个<script type="math/tex">\alpha</script>，则让输入通道数M变为<script type="math/tex">\alpha M</script>，输出通道数N变为<script type="math/tex">\alpha N</script>，<script type="math/tex">\alpha</script>的取值通常为1，0.75，0.5，0.25，使用了该参数之后的计算量为：</p>
<script type="math/tex; mode=display">
D_{K}\cdot D_{K}\cdot \alpha M\cdot D_{F}\cdot D_{F}+\alpha M\cdot \alpha N\cdot D_{F}\cdot D_{F}</script><p>计算量大约变成了以前的<script type="math/tex">\alpha^{2}</script></p>
<h3 id="3-4-Resolution-Multiplier-Reduced-Representation"><a href="#3-4-Resolution-Multiplier-Reduced-Representation" class="headerlink" title="3.4-Resolution Multiplier: Reduced Representation"></a>3.4-Resolution Multiplier: Reduced Representation</h3><p>第二个减少网络计算量的超参数是<script type="math/tex">\rho</script>(resolution multiplier)， 通过设置输入的分辨率来设置这一参数，然后内部的分辨率也会随之减少，在加上了超参数<script type="math/tex">\alpha,\rho</script>之后的计算量则变为：</p>
<script type="math/tex; mode=display">
D_{K}\cdot D_{K}\cdot \alpha M\cdot \rho D_{F}\cdot \rho D_{F}+\alpha M\cdot \alpha N\cdot \rho D_{F}\cdot \rho D_{F}</script><p>通常<script type="math/tex">\rho</script>设定之后的分辨率为224，192，160，128。注意在设定该参数之后计算量会发生变化，但是模型参数则不会发生变化。</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4-Experiments"></a>4-Experiments</h2><p>这部分主要讲了一些实验，首先是depthwise separable convolution，标准卷积的对比，瘦小和浅层MobileNet的对比。然后介绍了两个超参数的实验结果，包括ImageNet的准确度，Multi-Adds操作的数量和参数的数量。最后介绍了MobileNet在其它一些不同应用上（精细化分类，大尺度地理定位，人脸属性，目标检测，人脸嵌入）的实验结果。</p>
<h3 id="4-1-Model-Choices"><a href="#4-1-Model-Choices" class="headerlink" title="4.1-Model Choices"></a>4.1-Model Choices</h3><p>试验结果表明，全卷积的MobileNet和depthwise separable convolution相比，精度差不多，但是depthwise separable convolution的参数和计算量相比而言小了很多。瘦小和浅层MobileNet相比，计算量差不多的情况下瘦小MobileNet精度更高一些。实验结果如图：<img src="\images\paper-MobileNets\Model-Choices-Experiment.PNG" alt="关于模型选择的实验结果"></p>
<h3 id="4-2-Model-Shrinking-Hyperparameters"><a href="#4-2-Model-Shrinking-Hyperparameters" class="headerlink" title="4.2-Model Shrinking Hyperparameters"></a>4.2-Model Shrinking Hyperparameters</h3><p>这一部分是讲关于上述两个超参数调参的，其实验结果如下图所示：<img src="\images\paper-MobileNets\Model-Shrinking-Hyperparameters-experiment1.PNG" alt="超参数调参对比"></p>
<p><img src="\images\paper-MobileNets\Model-Shrinking-Hyperparameters-experiment2.PNG" alt="超参数调参对比"></p>
<p><img src="\images\paper-MobileNets\Model-Shrinking-Hyperparameters-experiment3.PNG" alt="与经典模型的对比"></p>
<h3 id="4-3-Fine-Grained-Recognition"><a href="#4-3-Fine-Grained-Recognition" class="headerlink" title="4.3-Fine Grained Recognition"></a>4.3-Fine Grained Recognition</h3><p>利用Stanford Dogs dataset数据集和网上的一些包含噪声的数据上训练了应用于精细化分类的模型，并且进行了很好的调参，最后在减小计算量和模型尺寸的情况下得到了近似于state of the art的结果，实验结果如图<img src="\images\paper-MobileNets\Fine-Grained-Recognition-experiment.PNG" alt="精细化分类的实验结果"></p>
<h3 id="4-4-Large-Scale-Geolocalizaton"><a href="#4-4-Large-Scale-Geolocalizaton" class="headerlink" title="4.4-Large Scale Geolocalizaton"></a>4.4-Large Scale Geolocalizaton</h3><p>PlaNet是将这个定位问题转化为一个分类问题来解决，PlaNet已经成功定位了很多照片，并且在这个问题上的表现已经超过了Im2GPS，用MobileNet结构在同样的数据下重新训练了PlaNet，实验结果如下图：<img src="\images\paper-MobileNets\Large-Scale-Geolocalizaton-experiment.PNG" alt="地理定位的实验结果"></p>
<h3 id="4-5-Face-Attributes"><a href="#4-5-Face-Attributes" class="headerlink" title="4.5-Face Attributes"></a>4.5-Face Attributes</h3><p>MobileNet还能够用于压缩未知训练过程的大规模系统，在人脸属性分类系统中使用了MobileNet和distillation的协同作用，在将二者进行结合之后，系统不仅不需要进行正则化，而且表现出来了更强的性能，实验结果如下图：<img src="\images\paper-MobileNets\Face-Attributes-experiment.PNG" alt="人脸属性检测的实验结果"></p>
<h3 id="4-6-Object-Detection"><a href="#4-6-Object-Detection" class="headerlink" title="4.6-Object Detection"></a>4.6-Object Detection</h3><p>这个实验利用VGG，Inception，MobileNet在SSD和Faster-RCNN上对COCO数据集进行了训练，结果如下图：<img src="\images\paper-MobileNets\Object-Detection-experiment.PNG" alt="目标检测的实验结果"></p>
<h3 id="4-7-Face-Embeddings"><a href="#4-7-Face-Embeddings" class="headerlink" title="4.7-Face Embeddings"></a>4.7-Face Embeddings</h3><p>FaceNet是FaceEmbedding的state of the art结果，这里同样的利用distillation来训练Mobile FaceNet。结果如下图：<img src="\images\paper-MobileNets\Face-Embeddings-experiment.PNG" alt="人脸嵌入的实验结果"></p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5-Conclusion"></a>5-Conclusion</h2><p>提出了基于depthwise separable convolution的模型结构，并且使用了width multiplier和resolution multiplier两个超参数来控制模型的复杂度，并且在模型尺寸，速度，准确度上面与其他模型进行了对比，证明了MobileNet在多种应用之下的高效性，下一步打算对MobileNet进行改进和进一步开发。</p>
<h2 id="6-其他的一些相关总结"><a href="#6-其他的一些相关总结" class="headerlink" title="6-其他的一些相关总结"></a>6-其他的一些相关总结</h2><p>数据集：ImageNet(图像分类)，Stanford Dogs dataset(精细化分类)，YFCC100M(人脸属性)，COCO(目标检测)</p>
<p>相关的论文：</p>
<p><strong>数据集</strong></p>
<p>《Imagenet large scale visual recognition challenge》（ImageNet，ILSVRC 2012）</p>
<p>《In First Workshop on Fine-Grained Visual Categorization》（Stanford Dogs dataset）</p>
<p>《Yfcc100m: The new data in multimedia research》（YFCC100M）</p>
<p><strong>更深更复杂精度更高的神经网络</strong></p>
<p>《Inception-v4,inception-resnet and the impact of residual connections onlearning》（InceptionV4）</p>
<p>《Rethinking the inception architecture for computer vision》（InceptionV3，空间维度的额外分解）</p>
<p>《Deep residual learning for image recognition》（resnet）</p>
<p>《Going deeper with convolutions》(GoogleNet)</p>
<p>《Very deep convolutional networks for large-scale image recognition》（VGG16）</p>
<p>《Imagenet classification with deep convolutional neural networks》（AlexNet）</p>
<p><strong>神经网络的压缩加速</strong></p>
<p>《Flattened convolutional neural networks for feedforward acceleration》（空间维度的额外分解）</p>
<p>《Factorized convolutional neural networks》（对卷积进行分解）</p>
<p>《Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 1mb model size》（利用bottleneck来实现小型网络）</p>
<p>《Quantized convolutional neural networks for mobile devices》（基于product quantization进行压缩）</p>
<p>《Xnornet:Imagenet classification using binary convolutional neural networks》（利用low bit networks）</p>
<p>《Training deep neural networks with low precision multiplications》（利用low bit networks）</p>
<p>《Quantized neural networks: Training neural networks with low precision weights and activations》（利用low bit networks）</p>
<p>《Xception: Deep learning with depthwise separable convolutions》（缩放depthwise separable filters）</p>
<p>《Structured transforms for small-footprint deep learning》（用于减少计算量的网络）</p>
<p>《Deep fried convnets》（用于减少计算量的网络）</p>
<p>《Compressing neural networks with the hashing trick》（利用哈希来压缩神经网络）</p>
<p>《Rigid-motion scattering for image classification》（最初提出标准卷积分解为depthwise conv和1x1 conv）</p>
<p>《Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding》（利用哈夫曼编码压缩网络）</p>
<p>《Speeding up convolutional neural networks with low rank expansions》（额外的变量分解）</p>
<p>《Speeding-up convolutional neural networks using fine-tuned cp-decomposition》（额外的变量分解）</p>
<p>《Distilling the knowledge in a neural network》（利用distillation来从大型网络训练小网络，以进行压缩）</p>
<p><strong>BN</strong></p>
<p>《Batch normalization: Accelerating deep network training by reducing internal covariate shift》（InceptionV2也从此而来）</p>
<p><strong>框架</strong></p>
<p>《Caffe: Convolutional architecture for fast feature embedding》</p>
<p>《Tensorflow: Large-scale machine learning on heterogeneous systems》</p>
<p><strong>图像定位</strong></p>
<p>《IM2GPS: estimating geographic information from a single image》（提出了Im2GPS）</p>
<p>《Large-Scale Image Geolocalization》（关于Im2GPS）</p>
<p>《PlaNet - Photo Geolocation with Convolutional Neural Networks》（PlaNet）</p>
<p><strong>精细化分类</strong></p>
<p>《The unreasonable effectiveness of noisy data for fine-grained recognition》</p>
<p><strong>目标检测</strong></p>
<p>《Faster r-cnn: Towards real-time object detection with region proposal networks》（Faster-RCNN框架）</p>
<p>《Ssd: Single shot multibox detector》（SSD框架）</p>
<p><strong>人脸嵌入</strong></p>
<p>《Facenet: A unified embedding for face recognition and clustering》（FaceNet，基于三元损失来构建人脸嵌入）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/10/04/paper-MobileNets/" data-id="cjomnvjq2000y40awh06y0z3b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-paper-InceptionV4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/28/paper-InceptionV4/" class="article-date">
  <time datetime="2018-09-28T09:47:58.000Z" itemprop="datePublished">2018-09-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/28/paper-InceptionV4/">[paper]InceptionV4总结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="《Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning》总结"><a href="#《Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning》总结" class="headerlink" title="《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》总结"></a>《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》总结</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>近些年，非常深的卷积神经网络在提升图像识别的性能表现上具有最大的促进作用。而Inception网络结构在具有很好的性能的同时还具有相对较低的计算消耗。最近的残差连接与传统结构的结合在2015 ILSVRC上取到了最好的结果，与InceptionV3的效果相近。考虑将Inception网络与残差连接相结合，充分的证据表明残差连接可以很大程度上加速Inception网络的训练，同样也有证据表明残差连接的Inception相比不带残差连接的几乎同样计算量的Inception网络性能要稍有优势。本文也提出了一些新的残差连接和不带残差连接的Inception网络，这些改变同样也明显改善了2012 ILSVRC的单帧分类性能。最后还提到了利用合适的激活缩放可以使非常宽的残差连接Inception网络的训练变得更加稳定</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>2012年的AlexNet在CV任务中取得了很好的成绩，深层的CNN在各种CV领域内取得了很成功的应用。因为残差连接在训练深层结构中是至关重要的，本文将残差连接和深层的Inception网络相结合，这样将在保持计算量的同时收获所有残差连接的优点。</p>
<p>除了直接的结合之外，本文还探索了是否可以将Inception自身做的更深和更宽来实现更好的性能，因此本文设计了InceptionV4，并由于Tensorflow的分布式计算的技术不再需要将模型进行划分。</p>
<p>本文还将Inception-V3，Inception-V4和同样计算消耗的残差连接的Inception网络进行了比较，很明显InceptionV4和Inception-ResNet-V2的单帧性能表现在ImageNet的验证集上相类似，都超过了state-of-the-art的结果。最后发现这种集成的性能也并没有达到数据集上分类噪声的程度，仍然还有改进的空间。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h2><p>CNN在大型的图片识别任务上变得很流行，Network in network，VGGNet和GoogLeNet(InceptionV1)都是一些重要的里程碑。He 给出了充分的理论和实践说明残差连接的优点，尤其是在检测上面的应用。作者强调残差连接在训练很深的CNN模型上是内在必要的，但是我们的发现并<strong>不支持</strong>这一观点，至少在图像识别这方面不支持，这大概需要更多的论点支撑和对残差连接在深层网络的作用的理解。实验的章节证明<strong>就算没有残差连接我们也不难训练很深的网络</strong>，不过<strong>残差连接能够很好的改善训练速度</strong>，这也是对于其使用的重要观点。</p>
<p>从InceptionV1开始该网络结构经过了多次改善，通过引入BN得到了V2，通过进行额外的因子分解得到了V3。</p>
<h2 id="3-Architectural-Choices"><a href="#3-Architectural-Choices" class="headerlink" title="3.Architectural Choices"></a>3.Architectural Choices</h2><p>这一章节主要介绍了网络的具体结构</p>
<p>在残差版本的Inception网络中使用了更加简单的Inception block，每个Inception block后面都有一个1*1的卷积来补偿通道数的改变。另外一个Inception和残差版本的小区别在于，在残差版本中，BN仅仅利用在传统层的顶部而不在summations的顶部，这是由于想让网络具有更多的Inception blocks，并可以在单个GPU上进行训练所做的妥协，事实上如果都使用BN会更友好处</p>
<p>当filters超过1000的时候，网络会在训练的早期就训练失败，降低学习率或者增加额外的BN都不能避免这一现象。He提出了利用先低学习率warm up，然后高学习率进行训练可以改善这一现象，然而本文发现通过对残差量进行缩放可以更加可靠的解决该问题，这样并不会损失准确性并且可以让训练更加稳定。<br><img src="\images\paper-InceptionV4\scaling.PNG" alt="残差变量的缩放"></p>
<h2 id="4-Training-Methodology"><a href="#4-Training-Methodology" class="headerlink" title="4.Training Methodology"></a>4.Training Methodology</h2><p>这一章节主要介绍了一些训练的细节，利用的tensorflow分布式计算框架，GPU为Nvidia的Kepler，最好的模型使用的RMSProp算法，dacay=0.9，<script type="math/tex">\epsilon=1.0</script>，学习率为0.045，以每两个epoch，0.94的指数衰减率进行衰减。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/28/paper-InceptionV4/" data-id="cjomnvjpx000p40awglvoi922" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Matrix-Derivative" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/21/Matrix-Derivative/" class="article-date">
  <time datetime="2018-09-21T08:19:52.000Z" itemprop="datePublished">2018-09-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/21/Matrix-Derivative/">[Matrix]关于向量与矩阵的求导</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="向量与矩阵的求导"><a href="#向量与矩阵的求导" class="headerlink" title="向量与矩阵的求导"></a>向量与矩阵的求导</h1><p>机器学习的算法中会遇到大量的与矩阵相关的微分与求导，在这里介绍一些常见的矩阵和向量相关的求导公式。</p>
<p>可以将常见的数据类型分为标量（scalar），向量（vector）和矩阵（matrix）这么几种，其中常见的求导运算分别为标量对标量/向量/矩阵的求导，向量对标量/向量的求导，矩阵对标量的求导这么几种，其他的求导计算的结果为张量，这里暂时不讨论。</p>
<p>对于求导结果的表示，分为分子布局（Numerator-layout notation）和分母布局（Denominator-layout notation）两种，这两种表示方式都是正确的，目前并没有相关的标准，最近看的《凸优化》上面通常采用分子布局的方式。</p>
<p>在分子布局的情况下，相关的求导维数如下表：<br><img src="\images\导数.PNG" alt="向量和矩阵求导的表格"></p>
<p>如果是向量与标量之间的求导，或者矩阵与标量之间的求导，求导结果即为矩阵或向量对标量的逐点求导得到的矩阵或向量，向量与向量之间的求导则会得到一个矩阵，称之为Jacobian matrix。</p>
<p>当向量对标量求导时为：<img src="\images\vector-scalar.svg" alt="向量对标量求导"><br>标量对向量求导时为：<img src="\images\scalar-vector.svg" alt="标量对向量求导"><br>矩阵与标量之间的求导的与上面的类似，当标量对矩阵求导时同样也需要转置。</p>
<p>当两个向量之间求导时，会得到一个雅各比矩阵：<img src="\images\vector-vector.svg" alt="向量对向量求导"></p>
<p>一些常见的矩阵求导公式如下：<br><img src="\images\vector-by-vector.PNG" alt="向量对向量的求导"><br><img src="\images\vector-by-scalar.PNG" alt="向量对标量的求导"><br><img src="\images\scalar-by-vector-1.PNG" alt="标量对向量的求导"><br><img src="\images\scalar-by-vector-2.PNG" alt="标量对向量的求导"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/21/Matrix-Derivative/" data-id="cjomnvjpo000d40awjzisxc3b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Matrix/">Matrix</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
<div class="widget-wrap">
  <h3 class="widget-title">ABOUT ME</h3>
  <ul class="widget about-me">
    
    <li><img class="author" title="About me" src="/images/cute.jpg" /></li>
    
    
    <li>Name：Tang Lang</li>
    
    <li>School：Xiamen University</li>
    
    <li>Email：langtang1996@gmail.com</li>
    
    <li>QQ：1660039482</li>
    
  </ul>
</div>


  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cpp/">Cpp</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matrix/">Matrix</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a><span class="tag-list-count">18</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/optimization/">optimization</a><span class="tag-list-count">9</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Cpp/" style="font-size: 15px;">Cpp</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/algorithm/" style="font-size: 12.5px;">algorithm</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/optimization/" style="font-size: 17.5px;">optimization</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/17/paper-dqn/">[paper]dqn</a>
          </li>
        
          <li>
            <a href="/2018/11/02/dataset-cityscapes/">dataset-cityscapes</a>
          </li>
        
          <li>
            <a href="/2018/10/30/paper-EDAnet/">[paper]EDAnet</a>
          </li>
        
          <li>
            <a href="/2018/10/22/paper-darts/">[paper]darts</a>
          </li>
        
          <li>
            <a href="/2018/10/15/paper-HashingTricks/">[paper]HashingTricks</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Tang Lang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>