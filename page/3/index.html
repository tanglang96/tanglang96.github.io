<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Math &amp; Code</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="the best or nothing">
<meta name="keywords" content="Algorithm">
<meta property="og:type" content="website">
<meta property="og:title" content="Math &amp; Code">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Math &amp; Code">
<meta property="og:description" content="the best or nothing">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Math &amp; Code">
<meta name="twitter:description" content="the best or nothing">
  
    <link rel="alternate" href="/atom.xml" title="Math &amp; Code" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Math &amp; Code</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">---By Frank,the best or nothing</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-最优化-求解线性方程组-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/20/最优化-求解线性方程组-1/" class="article-date">
  <time datetime="2018-07-20T13:17:02.000Z" itemprop="datePublished">2018-07-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/20/最优化-求解线性方程组-1/">[最优化]求解线性方程组(1)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="求解线性方程组（1）"><a href="#求解线性方程组（1）" class="headerlink" title="求解线性方程组（1）"></a>求解线性方程组（1）</h1><h2 id="最小二乘分析"><a href="#最小二乘分析" class="headerlink" title="最小二乘分析"></a>最小二乘分析</h2><p>在本文中将讨论线性方程组中的一种情况的求解，即考虑线性方程组</p>
<script type="math/tex; mode=display">
Ax=b</script><p>其中，<script type="math/tex">A\in R^{m*n},b\in R^{m},m\ge n,rank(A)=n</script>，在这种情况下，未知数的数量小于方程的数量，所以在很大可能上$b$不在$A$的值域空间中，即方程组无解，但是此时可以得到该线性方程方程的最小二乘解，即存在<script type="math/tex">x^{*}</script>使得对于所有的<script type="math/tex">x\in R^{n}</script>都有</p>
<script type="math/tex; mode=display">
||Ax-b||^{2}\ge ||Ax^{*}-b||^{2}</script><p>即称<script type="math/tex">x^{*}</script>为该线性方程的<strong>最小二乘解</strong>，当$b$在$A$的解空间中时，<script type="math/tex">x^{*}</script>自然就是该方程的解，最小二乘解可以通过以下公式直接计算出来：</p>
<script type="math/tex; mode=display">
x^{*}=(A^{T}A)^{-1}A^{T}b</script><p>证明过程如下：<br>构造目标函数</p>
<script type="math/tex; mode=display">f(x)=||Ax-b||^{2}\\
=(Ax-b)^{T}(Ax-b)\\
=\frac{1}{2}x^{T}(2A^{T}A)x-x^{T}(2A^{T}b)+b^{T}b</script><p>显然函数<script type="math/tex">f</script>为二次型函数，由于<script type="math/tex">rank(A)=n</script>，因此该二次型为正定二次型，利用局部极小点的一阶必要条件可以得到极小点满足</p>
<script type="math/tex; mode=display">
\nabla f(x)=2A^{T}Ax-2A^{T}b=0</script><p>该方程的唯一解为<script type="math/tex">x^{*}=(A^{T}A)^{-1}A^{T}b</script>，即为最小二乘解，最小二乘法对于直线拟合等应用是很方便的算法。</p>
<h2 id="递推最小二乘法"><a href="#递推最小二乘法" class="headerlink" title="递推最小二乘法"></a>递推最小二乘法</h2><p>上一节介绍了最小二乘法，我们可以用其来做直线拟合，如果要在拟合的数据中增加几组数据可以通过递推最小二乘法的方式来进行，即根据上次拟合的结果做部分修正即可，即利用上次拟合得到的最小二乘解<script type="math/tex">x^{*}</script>来得到数据点增加后的最小二乘解<script type="math/tex">x^{*}</script>。</p>
<p>某个优化问题为了寻找合适的<script type="math/tex">x</script>，使得<script type="math/tex">||A_{0}x-b^{(0)}||^{2}</script>最小，已知该问题的解为<script type="math/tex">x^{(0)}=G_{0}^{-1}A_{0}^{T}b^{(0)}</script>，其中<script type="math/tex">G_{0}=A^{T}_{0}A_{0}</script>，如果增加了新的数据，用矩阵<script type="math/tex">A_{1}</script>和向量<script type="math/tex">b_{1}</script>来表示，那么这个问题即为寻找<script type="math/tex">x</script>使得</p>
<script type="math/tex; mode=display">
||
\begin{bmatrix}
A_{0}\\A_{1}
\end{bmatrix}x-\begin{bmatrix}
b^{(0)}\\b^{(1)}
\end{bmatrix}||^{2}</script><p>达到最小<br>其迭代公式为：</p>
<script type="math/tex; mode=display">
G_{1}=G_{0}+A_{1}^{T}A_{1}\\
x^{(1)}=x^{(0)}+G_{1}^{-1}A_{1}^{T}(b^{(1)}-A_{1}x^{(0)})</script><p>证明过程暂略，在一般情况下的迭代公式为：</p>
<script type="math/tex; mode=display">
G_{k+}=G_{k}+A_{k+1}^{T}A_{k+1}\\
x^{(k+1)}=x^{(k)}+G_{k+1}^{-1}A_{k+1}^{T}(b^{(k+1)}-A_{k+1}x^{(k)})</script><p>对于线性方程组的求解，后面还将会介绍最小范数解和一般情况下的解法，以及伪逆等知识，To be continue…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/20/最优化-求解线性方程组-1/" data-id="cjmvacee3000q9wez9cdgoxqv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/optimization/">optimization</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-LeetCode-207-Course-Schedule" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/20/LeetCode-207-Course-Schedule/" class="article-date">
  <time datetime="2018-07-20T08:20:36.000Z" itemprop="datePublished">2018-07-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/20/LeetCode-207-Course-Schedule/">[LeetCode]207. Course Schedule</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>该题目利用DFS和BFS来判断某个图是否能进行拓扑排序</p>
<ol>
<li><p>BFS的思路是先将给定的pair组转换成vector<unordered_set>格式存放的图，然后计算每个节点的入度，先找到入度为0的节点，如果未找到直接返回false ，找到之后将该点的入度标记为-1，然后将其指向的节点的入度减1，循环n次，如果每次都能找到入度为0的节点，那么可以进行拓扑排序。</unordered_set></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canFinish</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph = make_graph(numCourses, prerequisites);</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; degrees = compute_indegree(graph);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numCourses; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (; j &lt; numCourses; j++)</span><br><span class="line">                <span class="keyword">if</span> (!degrees[j]) <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">if</span> (j == numCourses) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            degrees[j] = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> neigh : graph[j])</span><br><span class="line">                degrees[neigh]--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; make_graph(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph(numCourses);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> pre : prerequisites)</span><br><span class="line">            graph[pre.second].insert(pre.first);</span><br><span class="line">        <span class="keyword">return</span> graph;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; compute_indegree(<span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; graph) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; degrees(graph.size(), <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> neighbors : graph)</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> neigh : neighbors)</span><br><span class="line">                degrees[neigh]++;</span><br><span class="line">        <span class="keyword">return</span> degrees;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>DFS的思路则是通过DFS判断图中是否有环，通过onPath作为标记数组，来标记是否当前路径已经访问过，利用visited来标记已经访问过的节点，利用dfs_cycle()来判断当前的dfs路径上是否有环</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canFinish</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph = make_graph(numCourses, prerequisites);</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; onpath(numCourses, <span class="literal">false</span>), visited(numCourses, <span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numCourses; i++)</span><br><span class="line">            <span class="keyword">if</span> (!visited[i] &amp;&amp; dfs_cycle(graph, i, onpath, visited))</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; make_graph(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph(numCourses);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> pre : prerequisites)</span><br><span class="line">            graph[pre.second].insert(pre.first);</span><br><span class="line">        <span class="keyword">return</span> graph;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">dfs_cycle</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; graph, <span class="keyword">int</span> node, <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; onpath, <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; visited)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (visited[node]) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        onpath[node] = visited[node] = <span class="literal">true</span>; </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> neigh : graph[node])</span><br><span class="line">            <span class="keyword">if</span> (onpath[neigh] || dfs_cycle(graph, neigh, onpath, visited))</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span> onpath[node] = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/20/LeetCode-207-Course-Schedule/" data-id="cjmvacedk000a9wez2u0951ec" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/algorithm/">algorithm</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习-机器学习中的数值计算-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/14/机器学习-机器学习中的数值计算-1/" class="article-date">
  <time datetime="2018-07-14T10:05:22.000Z" itemprop="datePublished">2018-07-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/14/机器学习-机器学习中的数值计算-1/">[机器学习]机器学习中的数值计算(1)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="机器学习中的数值计算"><a href="#机器学习中的数值计算" class="headerlink" title="机器学习中的数值计算"></a>机器学习中的数值计算</h1><p>机器学习算法通常需要大量的数值计算，即通过迭代求解近似值而非求得解析解。这些算法通常包括最优化和线性方程组的求解，在计算机中要通过有限位来表示各种浮点数是具有一定误差的，需要通过一些方法来保证我们的计算精度。</p>
<h2 id="上溢与下溢"><a href="#上溢与下溢" class="headerlink" title="上溢与下溢"></a>上溢与下溢</h2><p><strong>上溢</strong>(overflow)是具有破坏力的，当大量级的数被近似为$-\infty$或者$\infty$时会发生上溢，这会导致在下一步的计算中，数字变成非数字。<br><strong>下溢</strong>(underflow)同样也是毁灭性的，即当浮点数很小时被四舍五入为0，有时候0和非0具有完全不同的性质，这样可能会导致分母为0等错误现象的发生。</p>
<p>可以通过softmax函数来避免上溢与下溢的发生：</p>
<script type="math/tex; mode=display">
softmax(\vec x)_{i}=\dfrac{exp(x_{i})}{\sum^{n}_{j=1}exp(x_{j})}</script><p>为了方便讨论，假设$\vec x$中的每个元素$x_{i}=c$，当$c$很大时，分子可能会上溢，同样的当$c$很小时分母会下溢。这两个问题可以通过计算$softmax(\vec z)$解决，其中$\vec  z=\vec x-max(x_{i})$，这样的话$\vec z$中的每个元素最大为0，分子不可能会发生上溢，至于分母，其中最少有一个值为1的分量，而其它元素都是非负的，所以分母也不可能发生下溢。这样便能解决溢出的问题。</p>
<h2 id="病态条件"><a href="#病态条件" class="headerlink" title="病态条件"></a>病态条件</h2><p>条件数指的是函数相对于输入的微小变化而变化的快慢程度。输入的轻微扰动而导致函数的迅速变化对于数值计算是有问题的。对于函数$f(x)=A^{-1}x$，当$A\in R^{n*n}$具有特征值分解的时候，其条件数为：</p>
<script type="math/tex; mode=display">
max_{i,j}|\frac{\lambda_{i}}{\lambda_{j}}|</script><p>即为最大特征值和最小特征值的比，当这个数很大时，其$f(x)$对于输入$x$的误差很敏感。</p>
<h2 id="基于梯度的优化"><a href="#基于梯度的优化" class="headerlink" title="基于梯度的优化"></a>基于梯度的优化</h2><p>深度学习算法通常使用基于梯度的优化算法，传统的梯度下降由于资料众多故不再赘述，在这里重点介绍关于梯度方法的其他知识，主要包括Jacobian矩阵和Hessian矩阵。</p>
<p>有时候我们需要计算输入输出都为向量的函数的所有偏导数，包含所有这样的偏导数的矩阵称为Jacobian矩阵。即如果有一个函数$f:R^{m}\to R^{n}$，$f$的Jacobian矩阵为$J_{i,j}=\frac{\partial}{\partial x_{j}}f(\vec x)_{i}$。</p>
<p>对于传统的梯度下降算法我们只用到了一阶导数，有时候在用到其他优化算法（比如牛顿法）需要用到二阶导数，将所有的二阶导数构成的矩阵称之为Hessian矩阵，即：</p>
<script type="math/tex; mode=display">
H(f)(\vec x)_{i,j}=\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}f(\vec x)</script><p>在深度学习的背景下，Hessian矩阵几乎处处是对称的。在特定方向$\vec d$上的二阶导数可以写成$\vec d^{T}H\vec d$。当$\vec d$是$H$的一个向量时，这个方向的二阶导数就是对应的特征值。</p>
<p>Hessian矩阵的一个重要作用就是进行二阶导数测试，即判断当前的点是极大/极小值点或者是鞍点。一维情况很简单，对于多维情况，我们通过检测Hessian矩阵的特征值来判断该临界点，当Hessian矩阵是正定时，该临界点是局部极小点，同样负定时则为局部极大点。如果Hessian的特征值中至少一个是正的一个是负的，那么$x$在某个平面上是局部极大点，在某个平面是局部极小点，则为鞍点。<br><img src="https://img-blog.csdn.net/20180714180718129?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>这是关于机器学习中数值计算文章系列的第一篇，To be continue…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/14/机器学习-机器学习中的数值计算-1/" data-id="cjmvacef5001k9wez8p9caioa" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习-利用TensorFlow训练一个简单的神经网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/07/06/机器学习-利用TensorFlow训练一个简单的神经网络/" class="article-date">
  <time datetime="2018-07-06T14:23:33.000Z" itemprop="datePublished">2018-07-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/07/06/机器学习-利用TensorFlow训练一个简单的神经网络/">[机器学习]利用TensorFlow训练一个简单的神经网络</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="利用TensorFlow训练一个简单的神经网络"><a href="#利用TensorFlow训练一个简单的神经网络" class="headerlink" title="利用TensorFlow训练一个简单的神经网络"></a>利用TensorFlow训练一个简单的神经网络</h1><p>我们在这里利用TensorFlow的Eager Execution 来构建模型，这样不用像以前一样创建Graph和Session了，可以使神经网络的训练更加方便快捷，下面以Iris数据集为例来训练一个神经网络，代码来自谷歌的教程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先导入相关的库</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import,division,print_function</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()	<span class="comment">#采用eager_execution</span></span><br><span class="line"><span class="comment">#查看版本信息并检查采用eager_execution是否打开</span></span><br><span class="line">print(<span class="string">'TensorFlow Version:&#123;&#125;'</span>.format(tf.VERSION))</span><br><span class="line">print(<span class="string">'Eager execution:&#123;&#125;'</span>.format(tf.executing_eagerly()))</span><br></pre></td></tr></table></figure></p>
<p>TensorFlow Version:1.8.0<br>Eager execution:True<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#获取数据集并显示在本地的保存位置</span><br><span class="line">train_dataset_url=&apos;http://download.tensorflow.org/data/iris_training.csv&apos;</span><br><span class="line">train_dataset_fp=tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),origin=train_dataset_url)</span><br><span class="line">print(&apos;Local copy of the dataset file:&#123;&#125;&apos;.format(train_dataset_fp))</span><br></pre></td></tr></table></figure></p>
<p>Downloading data from <a href="http://download.tensorflow.org/data/iris_training.csv" target="_blank" rel="noopener">http://download.tensorflow.org/data/iris_training.csv</a><br>8192/2194 [================================================================================================================] - 0s 0us/step<br>Local copy of the dataset file:C:\Users\Frank.keras\datasets\iris_training.csv<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对表格文件中的每一行进行解析，每行有5个元素，前面4个是特征，最后一个是标记</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_csv</span><span class="params">(line)</span>:</span></span><br><span class="line">    example_defaults=[[<span class="number">0.</span>],[<span class="number">0.</span>],[<span class="number">0.</span>],[<span class="number">0.</span>],[<span class="number">0</span>]]</span><br><span class="line">    parsed_line=tf.decode_csv(line,example_defaults)</span><br><span class="line">    features=tf.reshape(parsed_line[:<span class="number">-1</span>],shape=(<span class="number">4</span>,))</span><br><span class="line">    label=tf.reshape(parsed_line[<span class="number">-1</span>],shape=())</span><br><span class="line">    <span class="keyword">return</span> features,label</span><br><span class="line">	</span><br><span class="line">train_dataset=tf.data.TextLineDataset(train_dataset_fp)	<span class="comment">#读取csv转换为dataset</span></span><br><span class="line">train_dataset=train_dataset.skip(<span class="number">1</span>)	<span class="comment">#跳过标题行</span></span><br><span class="line">train_dataset=train_dataset.map(parse_csv)	<span class="comment">#对每一行都进行映射</span></span><br><span class="line">train_dataset=train_dataset.shuffle(buffer_size=<span class="number">1000</span>)	<span class="comment">#随机打乱</span></span><br><span class="line">train_dataset=train_dataset.batch(<span class="number">32</span>)	<span class="comment">#分批</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印一组训练数据</span></span><br><span class="line">features,label=iter(train_dataset).next()</span><br><span class="line">print(<span class="string">'example features:'</span>,features[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'example label:'</span>,label[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p>
<p>example features: tf.Tensor([6.8 3.  5.5 2.1], shape=(4,), dtype=float32)<br>example label: tf.Tensor(2, shape=(), dtype=int32)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立神经网络模型，两个隐藏层</span></span><br><span class="line">model=tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'relu'</span>,input_shape=(<span class="number">4</span>,)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">3</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数为softmax后的cross_entropy，返回一个损失函数的对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(model,x,y)</span>:</span></span><br><span class="line">    y_=model(x)</span><br><span class="line">    <span class="keyword">return</span> tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_)</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回一个梯度对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span><span class="params">(model,inputs,targets)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        loss_value=loss(model,inputs,targets)</span><br><span class="line">    <span class="keyword">return</span> tape.gradient(loss_value,model.variables)	<span class="comment">#返回梯度对象，传入损失函数和优化对象作为构造函数的参数</span></span><br><span class="line"></span><br><span class="line">optimizer=tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">train_loss_results=[]</span><br><span class="line">train_accuracy_results=[]</span><br><span class="line">num_epochs=<span class="number">201</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#优化过程迭代201次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    epoch_loss_avg=tfe.metrics.Mean()	<span class="comment">#交叉熵的平均误差对象</span></span><br><span class="line">    epoch_accuracy=tfe.metrics.Accuracy()	<span class="comment">#准确率对象</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> train_dataset:</span><br><span class="line">        grads=grad(model,x,y)</span><br><span class="line">        optimizer.apply_gradients(zip(grads, model.variables),	<span class="comment">#将梯度对应的模型变量分组</span></span><br><span class="line">                              global_step=tf.train.get_or_create_global_step())</span><br><span class="line">        epoch_loss_avg(loss(model,x,y))</span><br><span class="line">        epoch_accuracy(tf.argmax(model(x),axis=<span class="number">1</span>,output_type=tf.int32),y)</span><br><span class="line">    train_loss_results.append(epoch_loss_avg.result())</span><br><span class="line">    train_accuracy_results.append(epoch_accuracy.result())</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch &#123;:03d&#125;: Loss: &#123;:.3f&#125;, Accuracy: &#123;:.3%&#125;"</span>.format(epoch,epoch_loss_avg.result(),epoch_accuracy.result()</span><br></pre></td></tr></table></figure></p>
<p>Epoch 000: Loss: 1.217, Accuracy: 30.833%<br>Epoch 050: Loss: 0.524, Accuracy: 93.333%<br>Epoch 100: Loss: 0.261, Accuracy: 96.667%<br>Epoch 150: Loss: 0.169, Accuracy: 97.500%<br>Epoch 200: Loss: 0.133, Accuracy: 97.500%<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练过程损失函数和准确率的可视化																</span></span><br><span class="line">fig,axes=plt.subplots(<span class="number">2</span>,sharex=<span class="keyword">True</span>,figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">fig.suptitle(<span class="string">'Training Metrics'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">"Loss"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">0</span>].plot(train_loss_results)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>].set_ylabel(<span class="string">"Accuracy"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">"Epoch"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">1</span>].plot(train_accuracy_results)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdn.net/20180706222128445?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在测试集上测试模型表现</span></span><br><span class="line">test_url = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></span><br><span class="line"></span><br><span class="line">test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),</span><br><span class="line">                                  origin=test_url)</span><br><span class="line"></span><br><span class="line">test_dataset = tf.data.TextLineDataset(test_fp)</span><br><span class="line">test_dataset = test_dataset.skip(<span class="number">1</span>)</span><br><span class="line">test_dataset = test_dataset.map(parse_csv)</span><br><span class="line">test_dataset = test_dataset.shuffle(<span class="number">1000</span>)</span><br><span class="line">test_dataset = test_dataset.batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">test_accuracy = tfe.metrics.Accuracy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_dataset:</span><br><span class="line">  prediction = tf.argmax(model(x), axis=<span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">  test_accuracy(prediction, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set accuracy: &#123;:.3%&#125;"</span>.format(test_accuracy.result()))</span><br></pre></td></tr></table></figure></p>
<p>Test set accuracy: 100.000%<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用模型来进行预测</span></span><br><span class="line">class_ids = [<span class="string">"Iris setosa"</span>, <span class="string">"Iris versicolor"</span>, <span class="string">"Iris virginica"</span>]</span><br><span class="line">predict_dataset = tf.convert_to_tensor([</span><br><span class="line">    [<span class="number">5.1</span>, <span class="number">3.3</span>, <span class="number">1.7</span>, <span class="number">0.5</span>,],</span><br><span class="line">    [<span class="number">5.9</span>, <span class="number">3.0</span>, <span class="number">4.2</span>, <span class="number">1.5</span>,],</span><br><span class="line">    [<span class="number">6.9</span>, <span class="number">3.1</span>, <span class="number">5.4</span>, <span class="number">2.1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">predictions = model(predict_dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, logits <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">  class_idx = tf.argmax(logits).numpy()</span><br><span class="line">  name = class_ids[class_idx]</span><br><span class="line">  print(<span class="string">"Example &#123;&#125; prediction: &#123;&#125;"</span>.format(i, name))</span><br></pre></td></tr></table></figure></p>
<p>Example 0 prediction: Iris setosa<br>Example 1 prediction: Iris versicolor<br>Example 2 prediction: Iris virginica</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/07/06/机器学习-利用TensorFlow训练一个简单的神经网络/" data-id="cjmvaceew001d9wez9duh97ra" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习-在极客云上进行深度学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/29/机器学习-在极客云上进行深度学习/" class="article-date">
  <time datetime="2018-06-29T14:49:12.000Z" itemprop="datePublished">2018-06-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/29/机器学习-在极客云上进行深度学习/">[机器学习]在极客云上进行深度学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="极客云-深度学习云平台"><a href="#极客云-深度学习云平台" class="headerlink" title="极客云-深度学习云平台"></a>极客云-深度学习云平台</h1><p>最近在做老师给的一个图像相关的深度学习任务，代码调试后发现电脑内存不够(8g笔记本)，后来发现了一个很好用的深度学习云服务平台<br>极客云:<a href="http://www.jikecloud.net/" target="_blank" rel="noopener">http://www.jikecloud.net/</a><br><img src="https://img-blog.csdn.net/20180629223231171?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>这个平台最大的方便之处莫过于其自带很多计算框架，每次配置各种环境的时候都被折腾得要死，各种库之间的依赖关系实在头疼。并且该平台有很多不同配置的机器进行选择，价格也很公道，今天在我用了那个tesla p100的显卡，60g内存的机器之后，就再也没有出现我之前遇到的bad_alloc问题<br><img src="https://img-blog.csdn.net/20180629223343714?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h1 id="关于如何使用"><a href="#关于如何使用" class="headerlink" title="关于如何使用"></a>关于如何使用</h1><p>在进行训练之前可以先上传我们所使用的数据，我自己的数据大概10g左右，上传了一个晚上多一点，这个平台支持数据断点上传，并且在上传数据的时候是不会计算价格的，会默认放到/data目录下，最好是先解压上传压缩包上去，然后再在服务器上将数据解压到/input目录里，这样是最快的，因为貌似/data是机械盘，/input是固态。</p>
<p>数据上传之后可以创建一个服务器实例了，他会提供一个ssh登陆指令直接远程登陆操作就行，也可以使用jupyter notebook和tensorboard，很方便。也可以直接在独占/共享两种方式上切换，或者换一个性能更强大的服务器。<br><img src="https://img-blog.csdn.net/2018062922402981?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>看一下我这台机子的配置<br>Tesla p100的显卡，京东价5w多。。。<br><img src="https://img-blog.csdn.net/20180629224707639?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>4核8线的志强E5<br><img src="https://img-blog.csdn.net/2018062922475828?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>60g的内存<br><img src="https://img-blog.csdn.net/20180629224823880?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>需要注意的是，在服务器关机后/input和/output的数据会被清除，在关机之前要将这两个目录下所需要的数据转移到/data目录下，这样便可以关机后一直保存。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/06/29/机器学习-在极客云上进行深度学习/" data-id="cjmvacef3001i9wezq2ii47u2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
<div class="widget-wrap">
  <h3 class="widget-title">ABOUT ME</h3>
  <ul class="widget about-me">
    
    <li><img class="author" title="About me" src="/images/cute.jpg" /></li>
    
    
    <li>Name：Tang Lang</li>
    
    <li>School：Xiamen University</li>
    
    <li>Email：langtang1996@gmail.com</li>
    
    <li>QQ：1660039482</li>
    
  </ul>
</div>


  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cpp/">Cpp</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matrix/">Matrix</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/optimization/">optimization</a><span class="tag-list-count">9</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Cpp/" style="font-size: 15px;">Cpp</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/algorithm/" style="font-size: 12.5px;">algorithm</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/optimization/" style="font-size: 17.5px;">optimization</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/10/04/paper-MobileNets/">paper-MobileNets</a>
          </li>
        
          <li>
            <a href="/2018/09/28/paper-InceptionV4/">[paper]InceptionV4总结</a>
          </li>
        
          <li>
            <a href="/2018/09/21/Matrix-Derivative/">[Matrix]关于向量与矩阵的求导</a>
          </li>
        
          <li>
            <a href="/2018/09/14/leetcode-stock/">[LeetCode]动态规划中股票问题的通用解法</a>
          </li>
        
          <li>
            <a href="/2018/08/31/最优化-凸集的定义与常见凸集/">[最优化]凸集的定义与常见凸集</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Tang Lang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>