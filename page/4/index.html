<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Math &amp; Code</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="the best or nothing">
<meta name="keywords" content="Algorithm">
<meta property="og:type" content="website">
<meta property="og:title" content="Math &amp; Code">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="Math &amp; Code">
<meta property="og:description" content="the best or nothing">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Math &amp; Code">
<meta name="twitter:description" content="the best or nothing">
  
    <link rel="alternate" href="/atom.xml" title="Math &amp; Code" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Math &amp; Code</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">---By Frank,the best or nothing</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-机器学习-利用TensorFlow训练一个简单的神经网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/29/机器学习-利用TensorFlow训练一个简单的神经网络/" class="article-date">
  <time datetime="2018-08-29T10:43:05.000Z" itemprop="datePublished">2018-08-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/29/机器学习-利用TensorFlow训练一个简单的神经网络/">[机器学习]利用TensorFlow训练一个简单的神经网络</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="利用TensorFlow训练一个简单的神经网络"><a href="#利用TensorFlow训练一个简单的神经网络" class="headerlink" title="利用TensorFlow训练一个简单的神经网络"></a>利用TensorFlow训练一个简单的神经网络</h1><p>我们在这里利用TensorFlow的Eager Execution 来构建模型，这样不用像以前一样创建Graph和Session了，可以使神经网络的训练更加方便快捷，下面以Iris数据集为例来训练一个神经网络，代码来自谷歌的教程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先导入相关的库</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import,division,print_function</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()	<span class="comment">#采用eager_execution</span></span><br><span class="line"><span class="comment">#查看版本信息并检查采用eager_execution是否打开</span></span><br><span class="line">print(<span class="string">'TensorFlow Version:&#123;&#125;'</span>.format(tf.VERSION))</span><br><span class="line">print(<span class="string">'Eager execution:&#123;&#125;'</span>.format(tf.executing_eagerly()))</span><br></pre></td></tr></table></figure></p>
<p>TensorFlow Version:1.8.0<br>Eager execution:True<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#获取数据集并显示在本地的保存位置</span><br><span class="line">train_dataset_url=&apos;http://download.tensorflow.org/data/iris_training.csv&apos;</span><br><span class="line">train_dataset_fp=tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),origin=train_dataset_url)</span><br><span class="line">print(&apos;Local copy of the dataset file:&#123;&#125;&apos;.format(train_dataset_fp))</span><br></pre></td></tr></table></figure></p>
<p>Downloading data from <a href="http://download.tensorflow.org/data/iris_training.csv" target="_blank" rel="noopener">http://download.tensorflow.org/data/iris_training.csv</a><br>8192/2194 [================================================================================================================] - 0s 0us/step<br>Local copy of the dataset file:C:\Users\Frank.keras\datasets\iris_training.csv<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对表格文件中的每一行进行解析，每行有5个元素，前面4个是特征，最后一个是标记</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_csv</span><span class="params">(line)</span>:</span></span><br><span class="line">    example_defaults=[[<span class="number">0.</span>],[<span class="number">0.</span>],[<span class="number">0.</span>],[<span class="number">0.</span>],[<span class="number">0</span>]]</span><br><span class="line">    parsed_line=tf.decode_csv(line,example_defaults)</span><br><span class="line">    features=tf.reshape(parsed_line[:<span class="number">-1</span>],shape=(<span class="number">4</span>,))</span><br><span class="line">    label=tf.reshape(parsed_line[<span class="number">-1</span>],shape=())</span><br><span class="line">    <span class="keyword">return</span> features,label</span><br><span class="line">	</span><br><span class="line">train_dataset=tf.data.TextLineDataset(train_dataset_fp)	<span class="comment">#读取csv转换为dataset</span></span><br><span class="line">train_dataset=train_dataset.skip(<span class="number">1</span>)	<span class="comment">#跳过标题行</span></span><br><span class="line">train_dataset=train_dataset.map(parse_csv)	<span class="comment">#对每一行都进行映射</span></span><br><span class="line">train_dataset=train_dataset.shuffle(buffer_size=<span class="number">1000</span>)	<span class="comment">#随机打乱</span></span><br><span class="line">train_dataset=train_dataset.batch(<span class="number">32</span>)	<span class="comment">#分批</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印一组训练数据</span></span><br><span class="line">features,label=iter(train_dataset).next()</span><br><span class="line">print(<span class="string">'example features:'</span>,features[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'example label:'</span>,label[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p>
<p>example features: tf.Tensor([6.8 3.  5.5 2.1], shape=(4,), dtype=float32)<br>example label: tf.Tensor(2, shape=(), dtype=int32)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立神经网络模型，两个隐藏层</span></span><br><span class="line">model=tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'relu'</span>,input_shape=(<span class="number">4</span>,)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">3</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数为softmax后的cross_entropy，返回一个损失函数的对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(model,x,y)</span>:</span></span><br><span class="line">    y_=model(x)</span><br><span class="line">    <span class="keyword">return</span> tf.losses.sparse_softmax_cross_entropy(labels=y,logits=y_)</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回一个梯度对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span><span class="params">(model,inputs,targets)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        loss_value=loss(model,inputs,targets)</span><br><span class="line">    <span class="keyword">return</span> tape.gradient(loss_value,model.variables)	<span class="comment">#返回梯度对象，传入损失函数和优化对象作为构造函数的参数</span></span><br><span class="line"></span><br><span class="line">optimizer=tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">train_loss_results=[]</span><br><span class="line">train_accuracy_results=[]</span><br><span class="line">num_epochs=<span class="number">201</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#优化过程迭代201次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    epoch_loss_avg=tfe.metrics.Mean()	<span class="comment">#交叉熵的平均误差对象</span></span><br><span class="line">    epoch_accuracy=tfe.metrics.Accuracy()	<span class="comment">#准确率对象</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> train_dataset:</span><br><span class="line">        grads=grad(model,x,y)</span><br><span class="line">        optimizer.apply_gradients(zip(grads, model.variables),	<span class="comment">#将梯度对应的模型变量分组</span></span><br><span class="line">                              global_step=tf.train.get_or_create_global_step())</span><br><span class="line">        epoch_loss_avg(loss(model,x,y))</span><br><span class="line">        epoch_accuracy(tf.argmax(model(x),axis=<span class="number">1</span>,output_type=tf.int32),y)</span><br><span class="line">    train_loss_results.append(epoch_loss_avg.result())</span><br><span class="line">    train_accuracy_results.append(epoch_accuracy.result())</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch &#123;:03d&#125;: Loss: &#123;:.3f&#125;, Accuracy: &#123;:.3%&#125;"</span>.format(epoch,epoch_loss_avg.result(),epoch_accuracy.result()</span><br></pre></td></tr></table></figure></p>
<p>Epoch 000: Loss: 1.217, Accuracy: 30.833%<br>Epoch 050: Loss: 0.524, Accuracy: 93.333%<br>Epoch 100: Loss: 0.261, Accuracy: 96.667%<br>Epoch 150: Loss: 0.169, Accuracy: 97.500%<br>Epoch 200: Loss: 0.133, Accuracy: 97.500%<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练过程损失函数和准确率的可视化																</span></span><br><span class="line">fig,axes=plt.subplots(<span class="number">2</span>,sharex=<span class="keyword">True</span>,figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">fig.suptitle(<span class="string">'Training Metrics'</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">"Loss"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">0</span>].plot(train_loss_results)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>].set_ylabel(<span class="string">"Accuracy"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">"Epoch"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">1</span>].plot(train_accuracy_results)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://img-blog.csdn.net/20180706222128445?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在测试集上测试模型表现</span></span><br><span class="line">test_url = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></span><br><span class="line"></span><br><span class="line">test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),</span><br><span class="line">                                  origin=test_url)</span><br><span class="line"></span><br><span class="line">test_dataset = tf.data.TextLineDataset(test_fp)</span><br><span class="line">test_dataset = test_dataset.skip(<span class="number">1</span>)</span><br><span class="line">test_dataset = test_dataset.map(parse_csv)</span><br><span class="line">test_dataset = test_dataset.shuffle(<span class="number">1000</span>)</span><br><span class="line">test_dataset = test_dataset.batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">test_accuracy = tfe.metrics.Accuracy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_dataset:</span><br><span class="line">  prediction = tf.argmax(model(x), axis=<span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">  test_accuracy(prediction, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set accuracy: &#123;:.3%&#125;"</span>.format(test_accuracy.result()))</span><br></pre></td></tr></table></figure></p>
<p>Test set accuracy: 100.000%<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用模型来进行预测</span></span><br><span class="line">class_ids = [<span class="string">"Iris setosa"</span>, <span class="string">"Iris versicolor"</span>, <span class="string">"Iris virginica"</span>]</span><br><span class="line">predict_dataset = tf.convert_to_tensor([</span><br><span class="line">    [<span class="number">5.1</span>, <span class="number">3.3</span>, <span class="number">1.7</span>, <span class="number">0.5</span>,],</span><br><span class="line">    [<span class="number">5.9</span>, <span class="number">3.0</span>, <span class="number">4.2</span>, <span class="number">1.5</span>,],</span><br><span class="line">    [<span class="number">6.9</span>, <span class="number">3.1</span>, <span class="number">5.4</span>, <span class="number">2.1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">predictions = model(predict_dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, logits <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">  class_idx = tf.argmax(logits).numpy()</span><br><span class="line">  name = class_ids[class_idx]</span><br><span class="line">  print(<span class="string">"Example &#123;&#125; prediction: &#123;&#125;"</span>.format(i, name))</span><br></pre></td></tr></table></figure></p>
<p>Example 0 prediction: Iris setosa<br>Example 1 prediction: Iris versicolor<br>Example 2 prediction: Iris virginica</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/29/机器学习-利用TensorFlow训练一个简单的神经网络/" data-id="cjlfblpzd0015jgezkd9e6zh2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-机器学习-机器学习中的数值计算-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/29/机器学习-机器学习中的数值计算-1/" class="article-date">
  <time datetime="2018-08-29T10:43:03.000Z" itemprop="datePublished">2018-08-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/29/机器学习-机器学习中的数值计算-1/">[机器学习]机器学习中的数值计算(1)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="机器学习中的数值计算"><a href="#机器学习中的数值计算" class="headerlink" title="机器学习中的数值计算"></a>机器学习中的数值计算</h1><p>机器学习算法通常需要大量的数值计算，即通过迭代求解近似值而非求得解析解。这些算法通常包括最优化和线性方程组的求解，在计算机中要通过有限位来表示各种浮点数是具有一定误差的，需要通过一些方法来保证我们的计算精度。</p>
<h2 id="上溢与下溢"><a href="#上溢与下溢" class="headerlink" title="上溢与下溢"></a>上溢与下溢</h2><p><strong>上溢</strong>(overflow)是具有破坏力的，当大量级的数被近似为$-\infty$或者$\infty$时会发生上溢，这会导致在下一步的计算中，数字变成非数字。<br><strong>下溢</strong>(underflow)同样也是毁灭性的，即当浮点数很小时被四舍五入为0，有时候0和非0具有完全不同的性质，这样可能会导致分母为0等错误现象的发生。</p>
<p>可以通过softmax函数来避免上溢与下溢的发生：</p>
<script type="math/tex; mode=display">
softmax(\vec x)_{i}=\dfrac{exp(x_{i})}{\sum^{n}_{j=1}exp(x_{j})}</script><p>为了方便讨论，假设$\vec x$中的每个元素$x_{i}=c$，当$c$很大时，分子可能会上溢，同样的当$c$很小时分母会下溢。这两个问题可以通过计算$softmax(\vec z)$解决，其中$\vec  z=\vec x-max(x_{i})$，这样的话$\vec z$中的每个元素最大为0，分子不可能会发生上溢，至于分母，其中最少有一个值为1的分量，而其它元素都是非负的，所以分母也不可能发生下溢。这样便能解决溢出的问题。</p>
<h2 id="病态条件"><a href="#病态条件" class="headerlink" title="病态条件"></a>病态条件</h2><p>条件数指的是函数相对于输入的微小变化而变化的快慢程度。输入的轻微扰动而导致函数的迅速变化对于数值计算是有问题的。对于函数$f(x)=A^{-1}x$，当$A\in R^{n*n}$具有特征值分解的时候，其条件数为：</p>
<script type="math/tex; mode=display">
max_{i,j}|\frac{\lambda_{i}}{\lambda_{j}}|</script><p>即为最大特征值和最小特征值的比，当这个数很大时，其$f(x)$对于输入$x$的误差很敏感。</p>
<h2 id="基于梯度的优化"><a href="#基于梯度的优化" class="headerlink" title="基于梯度的优化"></a>基于梯度的优化</h2><p>深度学习算法通常使用基于梯度的优化算法，传统的梯度下降由于资料众多故不再赘述，在这里重点介绍关于梯度方法的其他知识，主要包括Jacobian矩阵和Hessian矩阵。</p>
<p>有时候我们需要计算输入输出都为向量的函数的所有偏导数，包含所有这样的偏导数的矩阵称为Jacobian矩阵。即如果有一个函数$f:R^{m}\to R^{n}$，$f$的Jacobian矩阵为$J_{i,j}=\frac{\partial}{\partial x_{j}}f(\vec x)_{i}$。</p>
<p>对于传统的梯度下降算法我们只用到了一阶导数，有时候在用到其他优化算法（比如牛顿法）需要用到二阶导数，将所有的二阶导数构成的矩阵称之为Hessian矩阵，即：</p>
<script type="math/tex; mode=display">
H(f)(\vec x)_{i,j}=\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}f(\vec x)</script><p>在深度学习的背景下，Hessian矩阵几乎处处是对称的。在特定方向$\vec d$上的二阶导数可以写成$\vec d^{T}H\vec d$。当$\vec d$是$H$的一个向量时，这个方向的二阶导数就是对应的特征值。</p>
<p>Hessian矩阵的一个重要作用就是进行二阶导数测试，即判断当前的点是极大/极小值点或者是鞍点。一维情况很简单，对于多维情况，我们通过检测Hessian矩阵的特征值来判断该临界点，当Hessian矩阵是正定时，该临界点是局部极小点，同样负定时则为局部极大点。如果Hessian的特征值中至少一个是正的一个是负的，那么$x$在某个平面上是局部极大点，在某个平面是局部极小点，则为鞍点。<br><img src="https://img-blog.csdn.net/20180714180718129?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZyYW5ra2tf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>这是关于机器学习中数值计算文章系列的第一篇，To be continue…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/29/机器学习-机器学习中的数值计算-1/" data-id="cjlfblpzk001cjgez9q0fsyns" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-LeetCode-207-Course-Schedule" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/29/LeetCode-207-Course-Schedule/" class="article-date">
  <time datetime="2018-08-29T10:43:01.000Z" itemprop="datePublished">2018-08-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/29/LeetCode-207-Course-Schedule/">[LeetCode]207. Course Schedule</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>该题目利用DFS和BFS来判断某个图是否能进行拓扑排序</p>
<ol>
<li><p>BFS的思路是先将给定的pair组转换成vector<unordered_set>格式存放的图，然后计算每个节点的入度，先找到入度为0的节点，如果未找到直接返回false ，找到之后将该点的入度标记为-1，然后将其指向的节点的入度减1，循环n次，如果每次都能找到入度为0的节点，那么可以进行拓扑排序。</unordered_set></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canFinish</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph = make_graph(numCourses, prerequisites);</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; degrees = compute_indegree(graph);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numCourses; i++) &#123;</span><br><span class="line">            <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (; j &lt; numCourses; j++)</span><br><span class="line">                <span class="keyword">if</span> (!degrees[j]) <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">if</span> (j == numCourses) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            degrees[j] = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> neigh : graph[j])</span><br><span class="line">                degrees[neigh]--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; make_graph(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph(numCourses);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> pre : prerequisites)</span><br><span class="line">            graph[pre.second].insert(pre.first);</span><br><span class="line">        <span class="keyword">return</span> graph;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; compute_indegree(<span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; graph) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; degrees(graph.size(), <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> neighbors : graph)</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> neigh : neighbors)</span><br><span class="line">                degrees[neigh]++;</span><br><span class="line">        <span class="keyword">return</span> degrees;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>DFS的思路则是通过DFS判断图中是否有环，通过onPath作为标记数组，来标记是否当前路径已经访问过，利用visited来标记已经访问过的节点，利用dfs_cycle()来判断当前的dfs路径上是否有环</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canFinish</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph = make_graph(numCourses, prerequisites);</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; onpath(numCourses, <span class="literal">false</span>), visited(numCourses, <span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numCourses; i++)</span><br><span class="line">            <span class="keyword">if</span> (!visited[i] &amp;&amp; dfs_cycle(graph, i, onpath, visited))</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; make_graph(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt; graph(numCourses);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> pre : prerequisites)</span><br><span class="line">            graph[pre.second].insert(pre.first);</span><br><span class="line">        <span class="keyword">return</span> graph;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">dfs_cycle</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; graph, <span class="keyword">int</span> node, <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; onpath, <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; visited)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (visited[node]) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        onpath[node] = visited[node] = <span class="literal">true</span>; </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> neigh : graph[node])</span><br><span class="line">            <span class="keyword">if</span> (onpath[neigh] || dfs_cycle(graph, neigh, onpath, visited))</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span> onpath[node] = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/29/LeetCode-207-Course-Schedule/" data-id="cjlfblpyh000ajgezxg4jsi0n" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/algorithm/">algorithm</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-最优化-求解线性方程组-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/29/最优化-求解线性方程组-1/" class="article-date">
  <time datetime="2018-08-29T10:42:58.000Z" itemprop="datePublished">2018-08-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/29/最优化-求解线性方程组-1/">[最优化]求解线性方程组(1)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="求解线性方程组（1）"><a href="#求解线性方程组（1）" class="headerlink" title="求解线性方程组（1）"></a>求解线性方程组（1）</h1><h2 id="最小二乘分析"><a href="#最小二乘分析" class="headerlink" title="最小二乘分析"></a>最小二乘分析</h2><p>在本文中将讨论线性方程组中的一种情况的求解，即考虑线性方程组</p>
<script type="math/tex; mode=display">
Ax=b</script><p>其中，<script type="math/tex">A\in R^{m*n},b\in R^{m},m\ge n,rank(A)=n</script>，在这种情况下，未知数的数量小于方程的数量，所以在很大可能上$b$不在$A$的值域空间中，即方程组无解，但是此时可以得到该线性方程方程的最小二乘解，即存在<script type="math/tex">x^{*}</script>使得对于所有的<script type="math/tex">x\in R^{n}</script>都有</p>
<script type="math/tex; mode=display">
||Ax-b||^{2}\ge ||Ax^{*}-b||^{2}</script><p>即称<script type="math/tex">x^{*}</script>为该线性方程的<strong>最小二乘解</strong>，当$b$在$A$的解空间中时，<script type="math/tex">x^{*}</script>自然就是该方程的解，最小二乘解可以通过以下公式直接计算出来：</p>
<script type="math/tex; mode=display">
x^{*}=(A^{T}A)^{-1}A^{T}b</script><p>证明过程如下：<br>构造目标函数</p>
<script type="math/tex; mode=display">f(x)=||Ax-b||^{2}\\
=(Ax-b)^{T}(Ax-b)\\
=\frac{1}{2}x^{T}(2A^{T}A)x-x^{T}(2A^{T}b)+b^{T}b</script><p>显然函数<script type="math/tex">f</script>为二次型函数，由于<script type="math/tex">rank(A)=n</script>，因此该二次型为正定二次型，利用局部极小点的一阶必要条件可以得到极小点满足</p>
<script type="math/tex; mode=display">
\nabla f(x)=2A^{T}Ax-2A^{T}b=0</script><p>该方程的唯一解为<script type="math/tex">x^{*}=(A^{T}A)^{-1}A^{T}b</script>，即为最小二乘解，最小二乘法对于直线拟合等应用是很方便的算法。</p>
<h2 id="递推最小二乘法"><a href="#递推最小二乘法" class="headerlink" title="递推最小二乘法"></a>递推最小二乘法</h2><p>上一节介绍了最小二乘法，我们可以用其来做直线拟合，如果要在拟合的数据中增加几组数据可以通过递推最小二乘法的方式来进行，即根据上次拟合的结果做部分修正即可，即利用上次拟合得到的最小二乘解<script type="math/tex">x^{*}</script>来得到数据点增加后的最小二乘解<script type="math/tex">x^{*}</script>。</p>
<p>某个优化问题为了寻找合适的<script type="math/tex">x</script>，使得<script type="math/tex">||A_{0}x-b^{(0)}||^{2}</script>最小，已知该问题的解为<script type="math/tex">x^{(0)}=G_{0}^{-1}A_{0}^{T}b^{(0)}</script>，其中<script type="math/tex">G_{0}=A^{T}_{0}A_{0}</script>，如果增加了新的数据，用矩阵<script type="math/tex">A_{1}</script>和向量<script type="math/tex">b_{1}</script>来表示，那么这个问题即为寻找<script type="math/tex">x</script>使得</p>
<script type="math/tex; mode=display">
||
\begin{bmatrix}
A_{0}\\A_{1}
\end{bmatrix}x-\begin{bmatrix}
b^{(0)}\\b^{(1)}
\end{bmatrix}||^{2}</script><p>达到最小<br>其迭代公式为：</p>
<script type="math/tex; mode=display">
G_{1}=G_{0}+A_{1}^{T}A_{1}\\
x^{(1)}=x^{(0)}+G_{1}^{-1}A_{1}^{T}(b^{(1)}-A_{1}x^{(0)})</script><p>证明过程暂略，在一般情况下的迭代公式为：</p>
<script type="math/tex; mode=display">
G_{k+}=G_{k}+A_{k+1}^{T}A_{k+1}\\
x^{(k+1)}=x^{(k)}+G_{k+1}^{-1}A_{k+1}^{T}(b^{(k+1)}-A_{k+1}x^{(k)})</script><p>对于线性方程组的求解，后面还将会介绍最小范数解和一般情况下的解法，以及伪逆等知识，To be continue…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/29/最优化-求解线性方程组-1/" data-id="cjlfblpyr000ijgezt70wvmj5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/optimization/">optimization</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-最优化-求解线性方程组-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/29/最优化-求解线性方程组-2/" class="article-date">
  <time datetime="2018-08-29T10:42:56.000Z" itemprop="datePublished">2018-08-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/08/29/最优化-求解线性方程组-2/">[最优化]求解线性方程组(2)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="求解线性方程组-2"><a href="#求解线性方程组-2" class="headerlink" title="求解线性方程组(2)"></a>求解线性方程组(2)</h1><h2 id="线性方程组的最小范数解"><a href="#线性方程组的最小范数解" class="headerlink" title="线性方程组的最小范数解"></a>线性方程组的最小范数解</h2><p>上一篇博文介绍了线性方程组的情况之一，即未知数数量小于方程个数的情况，介绍了最小二乘法，在本文中将介绍线性方程组的另一种情况，即方程个数小于未知数数量的情况，此时方程组有无限多的解，但是最接近原点的解，即范数最小的解只有一个，也就是这里将会介绍的线性方程组的<strong>最小范数解</strong>。</p>
<p>考虑线性方程组<script type="math/tex">Ax=b</script>，其中<script type="math/tex">A\in R^{m*n},m\le n</script>，要寻找其最小范数解，即相当于求解下列最优化问题：</p>
<script type="math/tex; mode=display">
minimize\ ||x||\\
subject\  to\ Ax=b</script><p>这个问题属于等式约束的最优化问题，可以利用拉格朗日乘子法求解，在这里我们介绍另外一种方法。</p>
<p>先给出结论，该方程组的最小范数解为<script type="math/tex">x^{*}=A^{T}(AA^{T})^{-1}b</script>，下面给出证明：</p>
<script type="math/tex; mode=display">
||x||^{2}=||(x-x^{*})+x^{*}||^{2}\\
=||x-x^{*}||^{2}+||x^{*}||^{2}+2x^{*T}(x-x^{*})</script><p>令<script type="math/tex">x^{*}=A^{T}(AA^{T})^{-1}b</script>代入上式最右边项可以得到<script type="math/tex">2x^{*T}(x-x^{*})=0</script>，于是<script type="math/tex">||x||^{2}=||x-x^{*}||^{2}+||x^{*}||^{2}</script>，<script type="math/tex">||x^{*}||^{2}</script>是一个定值，而当<script type="math/tex">x\neq x^{*}</script>时，<script type="math/tex">||x-x^{*}||^{2}>0</script>恒成立，因此可以得到<script type="math/tex">x=x^{*}</script>是该优化问题的唯一最小解，即最小范数解。</p>
<h2 id="Kaczmarz算法"><a href="#Kaczmarz算法" class="headerlink" title="Kaczmarz算法"></a>Kaczmarz算法</h2><p>Kaczmarz算法是一种迭代求解最小范数解的算法，可以省去求解<script type="math/tex">(AA^{T})^{-1}</script>的步骤，使计算更为高效，假设<script type="math/tex">A\in R^{m*n}</script>，直接给出迭代公式如下：<br>在前<script type="math/tex">m</script>次迭代中，即<script type="math/tex">k=0,1,...,m-1</script>时，有：</p>
<script type="math/tex; mode=display">
x^{(k+1)}=x^{(k)}+\mu(b_{k+1}-a_{k+1}^{T}x^{(k)})\frac{a_{k+1}}{a_{k+1}^{T}a_{k+1}}</script><p>对于第$(m+1)$次迭代，重新使用$A$的第1行以及$b$的第1个元素，即</p>
<script type="math/tex; mode=display">
x^{(m+1)}=x^{(m)}+\mu(b_{1}-a_{1}^{T}x^{(m)})\frac{a_{1}}{a_{1}^{T}a_{1}}</script><p>每迭代m次便从头循环一次，<script type="math/tex">\mu</script>可以视为迭代算法的过程。可以证明得到在Kaczmarz算法中，如果<script type="math/tex">x^{(0)}=0</script>，则当<script type="math/tex">k\to \infty</script>时，<script type="math/tex">x^{(k)}\to x^{*}=A^{T}(AA^{T})^{-1}b</script>，详细证明过程在这里省略。</p>
<p>后面的文章还会介绍线性方程组的一般解法，伪逆等相关内容，To be continue…</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/08/29/最优化-求解线性方程组-2/" data-id="cjlfblpyt000kjgezw5dn41bn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/optimization/">optimization</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/3/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cpp/">Cpp</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/optimization/">optimization</a><span class="tag-list-count">8</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Cpp/" style="font-size: 15px;">Cpp</a> <a href="/tags/algorithm/" style="font-size: 10px;">algorithm</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/optimization/" style="font-size: 20px;">optimization</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">24</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/08/29/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2018/08/29/Cpp-谈一谈const关键字/">[Cpp]谈一谈const关键字</a>
          </li>
        
          <li>
            <a href="/2018/08/29/最优化-线性规划概述/">[最优化]线性规划概述</a>
          </li>
        
          <li>
            <a href="/2018/08/29/最优化-求解线性规划问题的单纯形算法/">[最优化]求解线性规划问题的单纯形算法</a>
          </li>
        
          <li>
            <a href="/2018/08/29/Cpp-Cpp函数中的参数传递/">[Cpp]Cpp函数中的参数传递</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Tang Lang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>